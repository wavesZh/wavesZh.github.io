'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/','title':"Introduction",'content':" X Y  Z   "});index.add({'id':1,'href':'/tags/juc/','title':"juc",'content':""});index.add({'id':2,'href':'/posts/','title':"Posts",'content':""});index.add({'id':3,'href':'/tags/','title':"Tags",'content':""});index.add({'id':4,'href':'/tags/%E5%B9%B6%E5%8F%91/','title':"并发",'content':""});index.add({'id':5,'href':'/posts/juc-synchronized/','title':"深入理解JUC：synchronized",'content':"synchronized 关键字是 JAVA 中最基础的同步方式，以 JVM 底层的 monitor 监视器为基础实现的，但是由于使用 monitor 监视器会有上下文切换的损耗以及其他使用不便，JDK6 前推荐使用 LOCK 进行同步，随着 JVM 版本的升级，synchronized 得到了极大的优化：锁升级，锁粗化，锁消除等。\n基本使用 synchronized 可以修饰方法，也可以修饰对象。其根本的含义就是 “锁”。只是在不同的使用场景锁住的对象不一样而已。如果修饰静态方法，则锁住 class 对象；如果修饰非静态方法，则锁住 *class 实例对象；如果修饰代码块，则锁住监控对象。\n优化 锁粗化 将多次\n锁消除 逃逸分析，如果发现被锁的对象并不会发生逃逸后，将把锁消除。例如，如果\n对象头 在介绍锁升级之前，需要了解一个跟锁密切相关东西：对象头。其分为两部分：\n 存储对象自身的运行时数据，如 hashCode，GC 分代年龄等，也称作 Mark Word; 存储指向方法区对象类型（class type）的指针。  Mark Word 按照 JVM 定义占用一个字长，在32位JVM就占用32位，64位JVM占用64位，以32位为例，结构如下：\n|-------------------------------------------------------|--------------------| | Mark Word (32 bits) | State | |-------------------------------------------------------|--------------------| | identity_hashcode:25 | age:4 | biased_lock:1 | lock:2 | Normal | |-------------------------------------------------------|--------------------| | thread:23 | epoch:2 | age:4 | biased_lock:1 | lock:2 | Biased | |-------------------------------------------------------|--------------------| | ptr_to_lock_record:30 | lock:2 | Lightweight Locked | |-------------------------------------------------------|--------------------| | ptr_to_heavyweight_monitor:30 | lock:2 | Heavyweight Locked | |-------------------------------------------------------|--------------------| | | lock:2 | Marked for GC | |-------------------------------------------------------|--------------------| 状态 State 主要 biased_lock（偏向锁启用标志位） 和 lock（锁状态） 控制：\n   biased_lock lock State     0 01 无锁   1 01 偏向锁   0 00 轻量级锁   0 10 重量级锁   0 11 GC标记    epoch：偏向锁的年代信息 thread：持有偏向锁的线程 ID age：Java 对象年龄。 ptr_to_lock_record：指向栈中锁记录的指针 ptr_to_heavyweight_monitor：指向 monitor 的指针。\n偏向锁 偏向锁针对这种场景：锁不存在多线程竞争且总是由同一个线程多次获取，此时可以降低锁获取带来的开销。偏向锁需要通过启动参数（-XX:+UseBiasedLocking）开启。\n当某个线程首次获取锁时，若支持偏向锁，则会尝试 CAS 将 Mark Word 中的 thread 替换成当前线程 ID，并将 biased_lock 设置为1且 lock 是指为 01。操作成功后，持有偏向锁的线程之后每次进入这个锁相关的同步块，只需要做以下操作：\n 检查锁对象的 biased_lock 是否为 1， lock 是否为 01； 检查锁对象的 thread 是否为当前线程； 检查锁对象的 epoch 是否与 锁对象所属 class 对象的 epoch 是否一致。  同时满足以上条件才能认为线程持有偏向锁。\n如果锁对象的 thread 是不为当前线程且此线程不存在后，将撤销偏向锁，此时锁对象的 epoch 加一。锁的撤销操作比较昂贵，需避免频繁发生：需在安全点进行操作。当撤销次数达到指定阈值（-XX:BiasedLockingBulkRebiasThreshold）后，将进行批量重偏向，将该 class 对象的 epoch 加一，并且对该 class 对象所有正在使用偏向锁的锁对象 epoch 替换为 class 的 epoch。如果线程下一个尝试获取锁对象时发现其 epoch 小于 class 对象的 epoch，则说明此时持有偏向锁的线程并没有处理同步块，即便持有偏向锁线程存活且不为当前线程，当前线程也不需要进行撤销操作，可以直接 CAS 更新锁对象的 thread。如果批量重偏向次数达到指定阀值（-XX:BiasedLockingBulkRevokeThreshold）后，则会进行批量撤销：JVM 认为该 class 的使用场景存在多线程竞争，不允许再使用偏向锁，进而升级为轻量级锁。\n轻量级锁 实际应用中必然存在多线程获取同一个对象锁的场景。其设计主要是为了减少重量级锁使用系统互斥量产生的性能损耗。\n轻量级锁的加锁过程：\n 如果锁对象还未锁定，即 lock 为 01，则在当前线程的栈中建立锁记录（Lock Record）空间，其中部分空间（Displaced Mark Word）存储对象锁的 Mark Word 拷贝。 CAS 尝试将对象锁的 ptr_to_lock_record 更新为指向锁记录的指针，如更新成功，则代表当前线程成功持有锁。然后将对象锁的 lock 置为 00。 如果 CAS 更新失败，则检查对象锁的 ptr_to_lock_record 是否指向当前线程的栈帧。如果是，则应该是发生了重入，将锁记录中的 Displaced Mark Word 置空；如不是，则代表锁已被其他线程持有。轻量级锁将升级为重量级锁：lock 置为 10。  轻量级锁的解锁过程：\n如果对象锁中的 ptr_to_lock_record 指向当前线程的栈，则 CAS 将 Displaced Mark Word 替换到锁对象中的 Mark Word。如替换成功，则同步操作结束；若替换失败，则需升级为重量级锁并进入重量级锁的释放过程。\n由上可知，轻量级锁不能代替重量级锁，其只是在 “对于绝大部分的锁，在整个同步周期内都是不存在竞争的” 的条件下能避免了使用互斥量的开销。但是如果发生了竞争，除了互斥量的开销，还会有 CAS 带来的 CPU 开销，轻量级锁反而比重量级锁还要慢。\n重量级锁 重量级锁依赖操作系统的互斥锁机制实现的，但线程尝试获取重量级锁时，如果重量级锁已被其他线程持有，则该线程则会阻塞，直到重量级锁释放后被唤醒。\n由于操作重量级锁涉及内核态和用户态的切换，对并发性能带来了很大压力。为了避免这种操作，提出了自旋锁 技术：让尝试获取锁的线程自旋等待一段时间，看是否能获取到锁。这样就避免了线程切换的开销，但是其还是需要占用 CPU 时间。故而又提出自旋阀值，如果自旋次数超出阀值，则按原有方式挂起线程。再后来提出了自适应自旋锁，意味着自旋阀值不再固定，由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果同一个锁对象上自旋等待刚刚成功获得锁且持锁线程正在运行，则认为这次自旋很有可能再次成功。如果某锁对象很少自旋获得成功，则在以后可能直接略过自旋过程直接挂起，避免浪费 CPU 资源。\nlink *JVM-锁消除+锁粗化 自旋锁、偏向锁、轻量级锁 逃逸分析-30\n"});index.add({'id':6,'href':'/posts/juc-thread_pool_executor/','title':"深入理解JUC：ThreadPoolExecutor",'content':"线程池是 JUC 的核心组件之一，简化了并发执行的实现，使得我们不需要关心线程的管理以及任务分配，只需要向池里丢任务就行了。ThreadPoolExecutor 是线程池的核心实现。\n1. 核心思路 ThreadPoolExecutor 其实就是 生产-消费者 模式的实现。生产和消费动作通过内部的阻塞队列解耦。\n1.1 运行状态 通过 ctl 字段管理线程数量以及池的运行状态。 ctl 是 AtomicInteger 类型，int 原子操作类。高3位表示 runState 运行状态，低29位表示 workerCount 工作线程数，最大值为 2^29-1。\nrunState 提供生命周期的控制：\n RUNNING：可以接收新任务以及处理队列中的任务。-1 \u0026lt;\u0026lt; COUNT_BITS SHUTDOWN：不可以接收新任务，但可以处理队列中的任务。0 \u0026lt;\u0026lt; COUNT_BITS STOP：不可以接收新任务以及处理队列中的任务，并且中断正在处理的任务。1 \u0026lt;\u0026lt; COUNT_BITS TIDYING：所有任务已经终止且工作线程数为0，之后会调用 terminated() 方法。2 \u0026lt;\u0026lt; COUNT_BITS TERMINATED：terminated() 方法执行完后。3 \u0026lt;\u0026lt; COUNT_BITS  RUNNING -\u0026gt; SHUTDOWN：shutdown() 方法或者 finalize() 方法 (RUNNING OR SHUTDOWN) -\u0026gt; STOP：shutdownNow() 方法 STOP -\u0026gt; TIDYING：tryTerminate() 方法，当任务全部终止且工作线程数为0 TIDYING -\u0026gt; TERMINATED：tryTerminate() 方法，当 terminated() 方法执行完后\n以上状态的大小排序是递增的，以便根据大小进行状态流转判断。\n1.2 线程数量 corePoolSize 和 maximumPoolSize 字段限制了线程池中线程数量。maximumPoolSize 限制了线程池中允许存在线程的最大数量。超出 corePoolSize 数量的线程如果 keepAliveTime 时间内空闲则线程结束。allowCoreThreadTimeOut 默认是 false，corePoolSize 数量的线程会常驻池内，否则也将根据空闲时间进行结束。\n1.3 阻塞队列 workQueue 字段的类型是 BlockingQueue\u0026lt;Runnable\u0026gt;。为什么需要阻塞队列呢？\n 队列：存放待处理的任务，是实现生产-消费的关键； 阻塞：通过设置队列的大小限制允许存储任务的数量。  1.4 拒绝策略 由于工作线程和阻塞队列的数量限制或者 shutdown 后，有些任务不能执行，则需要 RejectedExecutionHandler 用于处理这些任务。\n2. 源码解析 2.1 提交任务 提交任务的方法有 execute 和 submit。submit 方法最终还是依托于 execute 方法实现。接下来介绍些 execute 方法：\npublic void execute(Runnable command) { if (command == null) throw new NullPointerException(); // 获取 ctrl 状态控制量  int c = ctl.get(); // 1. 工作线程数量 \u0026lt; corePoolSize  if (workerCountOf(c) \u0026lt; corePoolSize) { // 尝试增加新的 core worker 并让其执行任务  // 成功后退出，否则重新获取 ctrl  if (addWorker(command, true)) return; c = ctl.get(); } // executor 是否运行 \u0026amp;\u0026amp; 阻塞队列是否还可以容纳新任务  if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { // 重新获取 ctrl  int recheck = ctl.get(); // executor 是否运行，如果运行，则从阻塞队列中剔除该任务  if (! isRunning(recheck) \u0026amp;\u0026amp; remove(command)) // 执行拒绝策略  reject(command); // 增加非 core worker  else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 尝试增加非 core worker  else if (!addWorker(command, false)) // 新增失败则执行拒绝策略  reject(command); } 以上方法主要有3步：\n 当 workerCount 小于 corePoolSize 时，尝试新增 core 工作线程并交予任务。 当 executor 运行且任务可以放入 workQueue 时，二次检测 executor 运行状态。 当任务无法放入 workqueue 中，增尝试新增非 core 工作线程，失败则执行拒绝策略。  以上是大体处理任务流程\n2.2 新增工作线程 private boolean addWorker(Runnable firstTask, boolean core) { // 特殊跳转语法，不推荐使用  retry: // cas 修改 workerCount  for (;;) { int c = ctl.get(); // 运行状态  int rs = runStateOf(c); // rs == SHUTDOWN \u0026amp;\u0026amp; firstTask != null  // rs == SHUTDOWN \u0026amp;\u0026amp; workQueue.isEmpty()  // rs \u0026gt; SHUTDOWN  if (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; ! (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); // 工作线程数超过限制，直接退出 false  if (wc \u0026gt;= CAPACITY || wc \u0026gt;= (core ? corePoolSize : maximumPoolSize)) return false; // cas 修改 workerCount  if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // 二次检查 cas + loop  if (runStateOf(c) != rs) continue retry; // 由于工作线程数量发生改变，重试  } } // 创建并启动 worker  boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { int rs = runStateOf(ctl.get()); // 对比上面，rs == SHUTDOWN \u0026amp;\u0026amp; workQueue.isEmpty() 不允许启动线程  if (rs \u0026lt; SHUTDOWN || (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null)) { if (t.isAlive()) throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); // 记录 largestPoolSize 历史最大线程数  if (s \u0026gt; largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } if (workerAdded) { // 启动线程  t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted; } 新增工作线程主要有两个步骤：\n 修改工作线程数量 新增并启动工作线程  修改工作线程数量的前提条件，满足其中一个条件即可：\n rs \u0026lt; SHUTDOWN rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; !workQueue.isEmpty()  SHUTDOWN 状态下，不允许新增任务，但可以执行队列中的任务。 firstTask != null，违反 “不允许新增任务”； workQueue.isEmpty()，既然无待执行的任务，则没有必要 addWorker\n启动工作线程的前提条件跟以上类似，只是缺少了 \u0026amp; !workQueue.isEmpty()。逻辑问题而已，如果 rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; !workQueue.isEmpty() 成立， 那么 rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null 一定也成立。\n2.3 任务执行 任务丢进 workQueue 中后等待 Worker 消费执行。 任务执行的方法为 final void runWorker(Worker w)。\nfinal void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; // unlock() 允许中断，可查看 interruptIdleWorkers 方法  w.unlock(); boolean completedAbruptly = true; try { // 当没有待执行的任务时候，退出循环  while (task != null || (task = getTask()) != null) { // interruptIdleWorkers 方法无法中断已锁线程  w.lock(); // 检查线程的中断标志位。如线程池关闭，则需要确保中断标志位有效；如果状态\u0026lt;STOP, 则清除中断标志位。然而为了处理  // shutdownNow 方法与此时清除标志位的竞争问题，再次检查状态。确保中断标志位的准确性。  if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() \u0026amp;\u0026amp; runStateAtLeast(ctl.get(), STOP))) \u0026amp;\u0026amp; !wt.isInterrupted()) // 线程池关闭，设置中断标志位  wt.interrupt(); try { // before 回调  beforeExecute(wt, task); // 记录任务处理异常以便处理  Throwable thrown = null; try { // 执行任务  task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { // after 回调  afterExecute(task, thrown); } } finally { task = null; // 记录完成任务数  w.completedTasks++; w.unlock(); } } // completedAbruptly 是否有由于异常抛出退出  completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } } w.unlock() 代表者允许中断，具体解析将在 线程池关闭 章节说明。\n方法开启 while 循环执行待完成的任务，直至任务发生异常或者 task==null 时退出，即 worker 死亡。\n何时会发生 task==null，具体逻辑在 getTask() 方法：\n rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; (rs \u0026gt;= STOP || workQueue.isEmpty())) 成立。 (wc \u0026gt; maximumPoolSize || (timed \u0026amp;\u0026amp; timedOut)) \u0026amp;\u0026amp; (wc \u0026gt; 1 || workQueue.isEmpty()) \u0026amp;\u0026amp; compareAndDecrementWorkerCount(c) 成立。  第一点根据 SHUTDOWN 和 STOP 状态的含义很好理解。SHUTDOWN 允许继续消费队列中的任务，当队列中没有任务时，返回 null; STOP 则不允许，立即返回 null。 第二点主要是关于 worker 数量的控制，如 worker 数量超过 maximumPoolSize，则需要销毁多余的 worker。为了避免并发修改冲突，采用 CAS 的方式修改 worker 数量。但是销毁前须确保 workQueue 中剩余任务待完成。\n再看看其中的异常处理。异常包括用户异常和线程池内部异常。线程池内部异常都内部 catch，没有向外 throw。如 InterruptedException，此异常一般在线程池关闭时发生，具体可看 getTask() 方法。用户异常处理，将异常抛出，如为隐式异常需包装成显式异常，最终调用 processWorkerExit 方法后，worker 死亡。\nprivate void processWorkerExit(Worker w, boolean completedAbruptly) { // completedAbruptly 表示由于用户异常导致的退出 \tif (completedAbruptly) decrementWorkerCount(); // 操作 workers。从 workers 中剔除  final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { completedTaskCount += w.completedTasks; workers.remove(w); } finally { mainLock.unlock(); } // 尝试将状态变更为 Terminate  tryTerminate(); int c = ctl.get(); // 是否正常运行  if (runStateLessThan(c, STOP)) { if (!completedAbruptly) { // 内部异常处理，如果 worker 数量没有小于最低标准，需增加 worker  int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 \u0026amp;\u0026amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) \u0026gt;= min) return; // replacement not needed  } // 增加 worker  addWorker(null, false); } } 从上可知：\n 由用户异常导致 worker 死亡，线程池如正常运行则会重新新增一个 worker。\n 由内部异常导致 worker 死亡，线程池会根据 worker pool 最低标准来决定是否新增 worker。、\n  2.4 线程池关闭 线程池关闭有两个方法： shutdown 方法和 shutdownNow 方法。以上方法都只是尝试关闭，并不保证立即生效。\npublic void shutdown() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 检查 modifyThread 权限  checkShutdownAccess(); // 修改状态为 SHUTDOWN  advanceRunState(SHUTDOWN); // 中断空闲的 worker  interruptIdleWorkers(); // shutdown 回调  onShutdown(); // hook for ScheduledThreadPoolExecutor  } finally { mainLock.unlock(); } // 尝试修改状态为 Terminate  tryTerminate(); } shutdownNow 方法类似，就不贴了，套路如下：\n 检查 modifyThread 权限 修改 pool 状态 中断 worker tryTerminate     方法名称 状态变更 线程中断     shutdown SHUTDOWN 中断正在等待任务的线程   shutdownNow STOP 中断所有线程    中断正在等待任务的线程\nprivate void interruptIdleWorkers(boolean onlyOne) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { for (Worker w : workers) { Thread t = w.thread; //\t是否已经中断过 \u0026amp;\u0026amp; 是否允许中断  if (!t.isInterrupted() \u0026amp;\u0026amp; w.tryLock()) { try { t.interrupt(); } catch (SecurityException ignore) { } finally { w.unlock(); } } if (onlyOne) break; } } finally { mainLock.unlock(); } } 该方法如何判断空闲线程呢？ 关键在于 w.tryLock()。这跟 runWorker 方法中的 w.lock() 对应起来了。当调用 w.lock() 时，说明 worker 已经通过 getTask 方法获取到任务，以上的 t.interrupt() 无法执行。\n通过 t.interrupt() 可以标志 worker 需要中断了，但需要有中断响应的途径。如果 worker 阻塞在 workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) ，如果 workQueue 没有实现中断响应，也是只能乖乖等待超时。\n"});index.add({'id':7,'href':'/tags/kafka/','title':"kafka",'content':""});index.add({'id':8,'href':'/posts/kafka-consumer/','title':"Kafka 源码解析：KafkaConsumer 实现机制",'content':"Kafka 消费者负责从 Kafka 集群拉取消息\u0008进行消费以及提交 offset 偏移量。Kafka 定义了消费组的概念，消费组包含若干个普通消费者和一个 leader 消费者，leader 消费者会将 Topic 下的分区分配给自己以及其他消费者，原则是同一个 Topic 的一个分区只能分配给一个消费者。多个消费者并行消费 Topic 中的数据，提高消费性能。由于消费者-Topic-分区数量均可变，需要进行 Rebalance 重新分配以保证消息消费的均衡性。\n1. 主题订阅 一个消费者可以订阅多个主题\npublic void subscribe(Collection\u0026lt;String\u0026gt; topics, ConsumerRebalanceListener listener) { // 确保方法线程安全  acquireAndEnsureOpen(); try { // 检测 GroupId 是否设置  maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(\u0026#34;Topic collection to subscribe to cannot be null\u0026#34;); if (topics.isEmpty()) { // 列表为空，则表示取消订阅  this.unsubscribe(); } else { // 验证 topics 有效性  ······ // 检测是否分区分配方式，默认为 RangeAssignor  throwIfNoAssignorsConfigured(); // 清空非订阅分区拉取到的数据，避免消费错误  fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(\u0026#34;Subscribed to topic(s): {}\u0026#34;, Utils.join(topics, \u0026#34;, \u0026#34;)); // 订阅主题  if (this.subscriptions.subscribe(new HashSet\u0026lt;\u0026gt;(topics), listener)) // 标志需更新 主题元数据  metadata.requestUpdateForNewTopics(); } } finally { release(); } } KafkaConsumer 不是线程安全的，所以订阅时使用了 acquire方法和 release方法，根据当前持有者 currentThread和引用数refcount实现了线程安全以及可冲入性。\n当列表为空，取消订阅：\n 清除订阅关系以及拉取到的数据。 发送 LeaveGroupRequest 请求至\u0008 Kafka 服务端对应的 GroupCoordinator 表明消费者离开了消费组。  取消订阅需要通知 Kafka 服务端触发 Reblanace，以便服务端调整消费关系。\npublic synchronized void maybeLeaveGroup() { // 非静态成员 \u0026amp;\u0026amp; coordinator broker 实例有效 \u0026amp;\u0026amp; 之前加入过 Group  if (isDynamicMember() \u0026amp;\u0026amp; !coordinatorUnknown() \u0026amp;\u0026amp; state != MemberState.UNJOINED \u0026amp;\u0026amp; generation.hasMemberId()) { // 组装发送 LeaveGroupRequest 请求  log.info(\u0026#34;Member {} sending LeaveGroup request to coordinator {}\u0026#34;, generation.memberId, coordinator); LeaveGroupRequest.Builder request = new LeaveGroupRequest.Builder(new LeaveGroupRequestData() .setGroupId(groupId).setMemberId(generation.memberId)); client.send(coordinator, request) .compose(new LeaveGroupResponseHandler()); client.pollNoWakeup(); } // 重置年代信息  resetGeneration(); } 这里提到了静态成员，这是2.3提出的新概念，主要是为了减少 Reblanace。\n从KIP-345可知，静态成员一般不会发送 LeaveGroupRequest 请求。仅当以下4种情况触发 Reblanace：\n 新成员加入 leader rejoin 成员离线时间超过 session.timeout Broker 收到包含 group.instance.id 列表数据的 LeaveGroupRequest 请求  若列表不为空，则订阅主题，更新本地主题列表并且 needUpdate = true 标志需更新 Metadata 以更新服务端的订阅关系数据。\n主题订阅类型有3种\nprivate enum SubscriptionType { NONE, /** 按指定的主题订阅，自动分区 */ AUTO_TOPICS, /** 按正则匹配的主题订阅，自动分区 */ AUTO_PATTERN, /** 用户手动指定主题和分区 */ USER_ASSIGNED } 2. 拉取前提 KafkaConsumer 通过 poll 方法拉取消息，拥有多个重载方法。\nprivate ConsumerRecords\u0026lt;K, V\u0026gt; poll(final Timer timer, final boolean includeMetadataInTimeout) { acquireAndEnsureOpen(); try { // 是否指定订阅类型 SubscriptionType  if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) { throw new IllegalStateException(\u0026#34;Consumer is not subscribed to any topics or assigned any partitions\u0026#34;); } // 拉取数据直到超时  do { client.maybeTriggerWakeup(); // 分区分配所用时间是否包含载超时时间内  if (includeMetadataInTimeout) { // 更分配信息  if (!updateAssignmentMetadataIfNeeded(timer)) { return ConsumerRecords.empty(); } } else { // 使用一个新的 timer 阻塞直到分区分配完成  while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) { log.warn(\u0026#34;Still waiting for metadata\u0026#34;); } } final Map\u0026lt;TopicPartition, List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt;\u0026gt; records = pollForFetches(timer); if (!records.isEmpty()) { /// 流水线模式处理，拉取到消息后，继续发起请求，异步处理，等到下一次 pollForFetches，能更快拿到消息，提高处理速度  if (fetcher.sendFetches() \u0026gt; 0 || client.hasPendingRequests()) { // 异步发起请求  client.pollNoWakeup(); } return this.interceptors.onConsume(new ConsumerRecords\u0026lt;\u0026gt;(records)); } } while (timer.notExpired()); return ConsumerRecords.empty(); } finally { release(); } } 以上方法包含两个动作：分区分配以及拉取信息。includeMetadataInTimeout 参数决定分区分配的耗时是否包含在 timer 内。此外，由于 NIO 的便利，在拉取到数据后立即发起请求，不用阻塞等待响应结果，继续处理拉取到的数据，其他请求也在同步进行，形成一条流水线，提高处理效率。\n在拉取数据前，首先 updateAssignmentMetadataIfNeeded 方法需要确定分组以及拉取的 offset。\npublic boolean poll(Timer timer) { // 更新订阅信息  maybeUpdateSubscriptionMetadata(); // 回调处理 offset 提交的处理结果  invokeCompletedOffsetCommitCallbacks(); // 是否由用户手动指定主题分区  if (subscriptions.partitionsAutoAssigned()) { // 提供心跳，否则 HeartbeatThread 收到心跳，则认为消费者 dead，触发 Rebalance  pollHeartbeat(timer.currentTimeMs()); // 检测 broker 协调者有效性，是否能处理请求  if (coordinatorUnknown() \u0026amp;\u0026amp; !ensureCoordinatorReady(timer)) { return false; } // 是否需要 rejoin  if (rejoinNeededOrPending()) { ...... // 检测 Group 有效性  if (!ensureActiveGroup(timer)) { return false; } } } else { ... ... } // 如开启自动提交，则异步提交 offset  maybeAutoCommitOffsetsAsync(timer.currentTimeMs()); return true; } ConsumerCoordinator#poll 方法主要做了以下操作：\n 回调处理 offset 处理结果以及自动提交 offset； 心跳； 检查服务端 broker 协调者是否有效，确保能正常通信； 如订阅关系发生改变，确保 Group 有效性  ensureActiveGroup 方法确保分组有效，如分组无效，则触发 JoinGroup。\nboolean ensureActiveGroup(final Timer timer) { if (!ensureCoordinatorReady(timer)) { return false; } startHeartbeatThreadIfNeeded(); return joinGroupIfNeeded(timer); } 2.1 确保协调者有效 ensureCoordinatorReady 方法确保 broker 协调者有效以便通知\nprotected synchronized boolean ensureCoordinatorReady(final Timer timer) { // 检查协调者是否有效  if (!coordinatorUnknown()) return true; do { // 查找有效的 broker 协调者  final RequestFuture\u0026lt;Void\u0026gt; future = lookupCoordinator(); // 阻塞等待请求结果直至超时  client.poll(future, timer); if (!future.isDone()) { // 超时退出  break; } // 请求结果生异常  if (future.failed()) { // 是否为可重试异常  if (future.isRetriable()) { log.debug(\u0026#34;Coordinator discovery failed, refreshing metadata\u0026#34;); client.awaitMetadataUpdate(timer); } else throw future.exception(); } else if (coordinator != null \u0026amp;\u0026amp; client.isUnavailable(coordinator)) { // 请求结果获取的 coordinator 无效，则退避等待后重试  markCoordinatorUnknown(); timer.sleep(retryBackoffMs); } } while (coordinatorUnknown() \u0026amp;\u0026amp; timer.notExpired()); return !coordinatorUnknown(); } lookupCoordinator 方法根据最小活跃数负载均衡算法得到 broker 并向其发起 FindCoordinatorRequest 请求得到 Coordinator broker。\n2.2 确保分组有效 boolean joinGroupIfNeeded(final Timer timer) { // 是否需要 rejoin. 当订阅关系发生改变或者首次  while (rejoinNeededOrPending()) { // 确保 broker coordinator 有效  if (!ensureCoordinatorReady(timer)) { return false; } // needsJoinPrepare 标志，避免重复触发 onJoinPrepare 方法  if (needsJoinPrepare) { // join 前的回调操作  onJoinPrepare(generation.generationId, generation.memberId); needsJoinPrepare = false; } // 发起 JoinGroupRequest 请求  final RequestFuture\u0026lt;ByteBuffer\u0026gt; future = initiateJoinGroup(); client.poll(future, timer); if (!future.isDone()) { return false; } if (future.succeeded()) { // 拷贝主题分区信息  ByteBuffer memberAssignment = future.value().duplicate(); // join 完成后的回调操作  onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment); // 重置 JoinFuture, 以便下一次需要 join 时发起 JoinGroupRequest 请求  resetJoinGroupFuture(); needsJoinPrepare = true; } else { // 失败处理，重试或抛异常  ······ } } return true; } 发起 JoinGroupRequest 请求后的响应处理 JoinGroupResponseHandler#handle 方法\npublic void handle(JoinGroupResponse joinResponse, RequestFuture\u0026lt;ByteBuffer\u0026gt; future) { Errors error = joinResponse.error(); if (error == Errors.NONE) { log.debug(\u0026#34;Received successful JoinGroup response: {}\u0026#34;, joinResponse); sensors.joinLatency.record(response.requestLatencyMs()); synchronized (AbstractCoordinator.this) { // 检查 MemberState  if (state != MemberState.REBALANCING) { future.raise(new UnjoinedGroupException()); } else { // 设置 年代信息  AbstractCoordinator.this.generation = new Generation(joinResponse.data().generationId(), joinResponse.data().memberId(), joinResponse.data().protocolName()); // 判断当前消费者是否是消费组里面的 leader  if (joinResponse.isLeader()) { // leader 操作， joinResponse 统计了所有消费者，以便 leader 分配分区  onJoinLeader(joinResponse).chain(future); } else { // follower 操作  onJoinFollower().chain(future); } } } } // 异常处理省略  ······ } MemberState 有三个状态：\nprivate enum MemberState { UNJOINED, // 客户端还没有加入消费组  REBALANCING, // 客户端正准备 Rebalance  STABLE, // 客户端已经加入消费者并已稳定运行 } JoinGroupRequest 请求实际是为了确认消费组中的消费者 leader。onJoinLeader 方法将按照分区分配策略得到分配结果，发起 SyncGroupRequest 请求将分配结果同步给服务端； onJoinFollower 方法则只是发起 SyncGroupRequest 请求得到分区分配结果。\n JoinGroupRequest: 服务端收集消费者信息并确认 leader; SyncGroupRequest: leader 将分区分配信息同步至服务端； 服务端同步分区分配信息至消费者。\n 2.3 确保 offset 有效 然后回到 KafkaConsumer#updateAssignmentMetadataIfNeeded 方法， coordinator.poll 执行后保证了分区分配信息的有效性，接下来要获取拉取 offset 的有效性了\nprivate boolean updateFetchPositions(final Timer timer) { // 验证变更的leader的分配分区的偏移量  fetcher.validateOffsetsIfNeeded(); // 检测是否分区都有有效的 offset  cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions(); if (cachedSubscriptionHashAllFetchPositions) return true; // 刷新 offset  if (coordinator != null \u0026amp;\u0026amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) return false; // 刷新后仍没有有效 offset,设置重置策略  subscriptions.resetMissingPositions(); // 发起 ListOffsetRequest 请求以重置 offset  fetcher.resetOffsetsIfNeeded(); return true; } 获取拉取 offset 一般是2步：\n 发起 OffsetFetchRequest 请求获取每个分区的 offset 信息； 当某个分区没有 offset 时，执行 offset reset 策略。  offset reset 策略有2种\n LATEST: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据；在 Kafka 分区则表示把位移调整到分区当前最小位移 EARLIEST: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费；在 Kafka 分区则表示把位移调整到分区当前最新位移  3. 拉取消息 以上操作完成后，即可开始拉取消息。回到 KafkaConsumer.poll 方法，其中 ´pollForFetches 方法负责拉取数据。\npublic synchronized int sendFetches() { sensors.maybeUpdateAssignment(subscriptions); // 针对需要拉取的分区创建拉取请求  Map\u0026lt;Node, FetchSessionHandler.FetchRequestData\u0026gt; fetchRequestMap = prepareFetchRequests(); // Node 为可读分区的 broker 或者 leader broker  for (Map.Entry\u0026lt;Node, FetchSessionHandler.FetchRequestData\u0026gt; entry : fetchRequestMap.entrySet()) { final Node fetchTarget = entry.getKey(); final FetchSessionHandler.FetchRequestData data = entry.getValue(); // 组装 FetchRequest 请求  final FetchRequest.Builder request = FetchRequest.Builder .forConsumer(this.maxWaitMs, this.minBytes, data.toSend()) .isolationLevel(isolationLevel) .setMaxBytes(this.maxBytes) .metadata(data.metadata()) .toForget(data.toForget()) .rackId(clientRackId); if (log.isDebugEnabled()) { log.debug(\u0026#34;Sending {} {} to broker {}\u0026#34;, isolationLevel, data.toString(), fetchTarget); } //发送 FetchRequest 请求  RequestFuture\u0026lt;ClientResponse\u0026gt; future = client.send(fetchTarget, request); this.nodesWithPendingFetchRequests.add(entry.getKey().id()); // 注册响应处理  future.addListener(new RequestFutureListener\u0026lt;ClientResponse\u0026gt;() { // 省略响应处理，待会再解析  ······ } } // 返回请求数  return fetchRequestMap.size(); } 3.1 确认拉取分区 prepareFetchRequests 方法创建了拉取请求所需要的数据，从返回值为 Map\u0026lt;Node, FetchSessionHandler.FetchRequestData\u0026gt; 可看出，请求是以 Node 为区分的，一个 Node 只会请求一次，返回的数据可能包含了多个主题或多个分区的数据。\nfetchablePartitions 方法： 如仍有数据待消费的分区，不需要再次 Fetch。\nthis.nodesWithPendingFetchRequests.contains(node.id()) ： 如有待响应 Fetch 请求的分区，不需要再次 Fetch。\n在历史版本中，经过上面的过滤后的分区则是最终需要拉取的。但是 KIP-227 中介绍了这种 Fetch 方式的低效原因：\n There are two major inefficiencies in the current FetchRequest paradigm. The first one is that the set of partitions which the follower is interested in changes only rarely. Yet each FetchRequest must enumerate the full set of partitions which the follower is interested in. The second inefficiency is that even when nothing has changed in a partition since the previous FetchRequest, we must still send back metadata about that partition.\n 感觉概括来说，对没有新消息的分区仍不断请求，broker仍返回改分区的元数据。针对这种场景，Kafka 提出了 FetchSession 避免将此作为每个 Fetch 请求的一部分重新发送。\nprepareFetchRequests 方法有三个条件分支：\n node 是否存在 对应 node 分区是否存在待响应的 fetch 请求 准备 fetch 请求数据  以下是分支3的主要操作:\npublic FetchRequestData build() { // 是否全量请求  if (nextMetadata.isFull()) { if (log.isDebugEnabled()) { log.debug(\u0026#34;Built full fetch {} for node {} with {}.\u0026#34;, nextMetadata, node, partitionsToLogString(next.keySet())); } // 设置 fetch session 的分区  sessionPartitions = next; next = null; Map\u0026lt;TopicPartition, PartitionData\u0026gt; toSend = Collections.unmodifiableMap(new LinkedHashMap\u0026lt;\u0026gt;(sessionPartitions)); return new FetchRequestData(toSend, Collections.emptyList(), toSend, nextMetadata); } // 部分请求/增量请求  // session 中 新增/删除/调整的分区  List\u0026lt;TopicPartition\u0026gt; added = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;TopicPartition\u0026gt; removed = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;TopicPartition\u0026gt; altered = new ArrayList\u0026lt;\u0026gt;(); for (Iterator\u0026lt;Entry\u0026lt;TopicPartition, PartitionData\u0026gt;\u0026gt; iter = sessionPartitions.entrySet().iterator(); iter.hasNext(); ) { Entry\u0026lt;TopicPartition, PartitionData\u0026gt; entry = iter.next(); TopicPartition topicPartition = entry.getKey(); PartitionData prevData = entry.getValue(); PartitionData nextData = next.get(topicPartition); if (nextData != null) { if (prevData.equals(nextData)) { // offset 跟上一次拉取时一致，此次不再拉取  next.remove(topicPartition); } else { // 调整分区位置  next.remove(topicPartition); next.put(topicPartition, nextData); entry.setValue(nextData); altered.add(topicPartition); } } else { // 将分区从 session 中删除  iter.remove(); removed.add(topicPartition); } } // 查找 next 中 sessionPartitions 没有的分区  for (Entry\u0026lt;TopicPartition, PartitionData\u0026gt; entry : next.entrySet()) { TopicPartition topicPartition = entry.getKey(); PartitionData nextData = entry.getValue(); if (sessionPartitions.containsKey(topicPartition)) { // 经过上面的循环后，sessionPartitions 和 next 共有的元素要么被删除了，要么移动至末尾，  // 如果此时再次遇到共有元素，说明后面没有新的元素了  break; } sessionPartitions.put(topicPartition, nextData); added.add(topicPartition); } if (log.isDebugEnabled()) { log.debug(\u0026#34;Built incremental fetch {} for node {}. Added {}, altered {}, removed {} \u0026#34; + \u0026#34;out of {}\u0026#34;, nextMetadata, node, partitionsToLogString(added), partitionsToLogString(altered), partitionsToLogString(removed), partitionsToLogString(sessionPartitions.keySet())); } Map\u0026lt;TopicPartition, PartitionData\u0026gt; toSend = Collections.unmodifiableMap(new LinkedHashMap\u0026lt;\u0026gt;(next)); Map\u0026lt;TopicPartition, PartitionData\u0026gt; curSessionPartitions = Collections.unmodifiableMap(new LinkedHashMap\u0026lt;\u0026gt;(sessionPartitions)); next = null; // 封装数据  return new FetchRequestData(toSend, Collections.unmodifiableList(removed), curSessionPartitions, nextMetadata); } sessionPartitions 只有3个场景进行了调整：\n 全量拉取时初始化/赋值； sessionPartitions 含有 next 没有的分区时剔除分区； next 中含有 sessionPartitions 没有的分区时新增分区。  场景2的触发可能是 prepareFetchRequests 处理中的前2步过滤：待消费数据的分区以及待响应的 fetch 请求所属分区。 场景3的触发可能是由于场景2剔除的分区，后面拉取通过了前2步过滤。\n以上只是客户端所做出的优化，需要服务的配套程序进行响应处理，还没看\u0026hellip;\n3.2 Fetch 请求响应 future.addListener(new RequestFutureListener\u0026lt;ClientResponse\u0026gt;() { @Override public void onSuccess(ClientResponse resp) { synchronized (Fetcher.this) { try { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) FetchResponse\u0026lt;Records\u0026gt; response = (FetchResponse\u0026lt;Records\u0026gt;) resp.responseBody(); // 获取 node 对应的 FetchSessionHandler  FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler == null) { log.error(\u0026#34;Unable to find FetchSessionHandler for node {}. Ignoring fetch response.\u0026#34;, fetchTarget.id()); return; } // 校验数据以及更新 Fetch Session 相关信息  if (!handler.handleResponse(response)) { return; } Set\u0026lt;TopicPartition\u0026gt; partitions = new HashSet\u0026lt;\u0026gt;(response.responseData().keySet()); FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions); for (Map.Entry\u0026lt;TopicPartition, FetchResponse.PartitionData\u0026lt;Records\u0026gt;\u0026gt; entry : response.responseData().entrySet()) { TopicPartition partition = entry.getKey(); FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition); if (requestData == null) { String message; if (data.metadata().isFull()) { message = MessageFormatter.arrayFormat( \u0026#34;Response for missing full request partition: partition={}; metadata={}\u0026#34;, new Object[]{partition, data.metadata()}).getMessage(); } else { message = MessageFormatter.arrayFormat(\u0026#34;Response for missing session request partition: partition={}; metadata={}; toSend={}; toForget={}\u0026#34;, new Object[]{partition, data.metadata(), data.toSend(), data.toForget()}).getMessage(); } // Received fetch response for missing session partition  throw new IllegalStateException(message); } else { long fetchOffset = requestData.fetchOffset; FetchResponse.PartitionData\u0026lt;Records\u0026gt; fetchData = entry.getValue(); log.debug(\u0026#34;Fetch {} at offset {} for partition {} returned fetch data {}\u0026#34;, isolationLevel, fetchOffset, partition, fetchData); // 封装为 CompletedFetch 丢入队列中  completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator, resp.requestHeader().apiVersion())); } } sensors.fetchLatency.record(resp.requestLatencyMs()); } finally { nodesWithPendingFetchRequests.remove(fetchTarget.id()); } } } }); 响应处理主要是将拉取到的数据封装为 CompletedFetch 丢入队列中，等待下一次 poll 消费 (fetchedRecords 方法)。\n3.3 提取数据 public Map\u0026lt;TopicPartition, List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt;\u0026gt; fetchedRecords() { Map\u0026lt;TopicPartition, List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt;\u0026gt; fetched = new HashMap\u0026lt;\u0026gt;(); // 限制最大拉取消息数，`max.poll.records`，由此可以控制消费速率  int recordsRemaining = maxPollRecords; try { while (recordsRemaining \u0026gt; 0) { // 由于最大拉取消息数的限制，之前还未处理完的消息  if (nextInLineRecords == null || nextInLineRecords.isFetched) { // peek 方法不移除数据  CompletedFetch completedFetch = completedFetches.peek(); if (completedFetch == null) break; try { // fetch done 回调  nextInLineRecords = parseCompletedFetch(completedFetch); } catch (Exception e) { // fetched.isEmpty() 确保不会因为异常导致数据丢失  // partition.records == null || partition.records.sizeInBytes() == 0 无效数据可以丢弃  FetchResponse.PartitionData partition = completedFetch.partitionData; if (fetched.isEmpty() \u0026amp;\u0026amp; (partition.records == null || partition.records.sizeInBytes() == 0)) { completedFetches.poll(); } throw e; } completedFetches.poll(); } else { // 提取数据并更新本地 offset  List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt; records = fetchRecords(nextInLineRecords, recordsRemaining); TopicPartition partition = nextInLineRecords.partition; if (!records.isEmpty()) { List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt; currentRecords = fetched.get(partition); if (currentRecords == null) { fetched.put(partition, records); } else { // this case shouldn\u0026#39;t usually happen because we only send one fetch at a time per partition,  // but it might conceivably happen in some rare cases (such as partition leader changes).  // we have to copy to a new list because the old one may be immutable  List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt; newRecords = new ArrayList\u0026lt;\u0026gt;(records.size() + currentRecords.size()); newRecords.addAll(currentRecords); newRecords.addAll(records); fetched.put(partition, newRecords); } // 更新剩余拉取信息数  recordsRemaining -= records.size(); } } } } catch (KafkaException e) { // 防止因 KafkaException 而丢弃数据  if (fetched.isEmpty()) throw e; } return fetched; }public Map\u0026lt;TopicPartition, List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt;\u0026gt; fetchedRecords() { // 异常处理  ······ Map\u0026lt;TopicPartition, List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt;\u0026gt; drained = new HashMap\u0026lt;\u0026gt;(); // 限制最大拉取消息数，`max.poll.records`，由此可以控制消费速率  int recordsRemaining = maxPollRecords; while (recordsRemaining \u0026gt; 0) { if (nextInLineRecords == null || nextInLineRecords.isDrained()) { // 从队列中拉取数据，每个 completedFetch 都是一个分区内的数据集  CompletedFetch completedFetch = completedFetches.poll(); // 队列无数据后，退出循环，返回数据  if (completedFetch == null) break; try { // 解析信息，反序列化以及处理服务端返回的错误码  nextInLineRecords = parseCompletedFetch(completedFetch); } catch (KafkaException e) { if (drained.isEmpty()) throw e; // 记录异常  nextInLineExceptionMetadata = new ExceptionMetadata(completedFetch.partition, completedFetch.fetchedOffset, e); } } else { TopicPartition partition = nextInLineRecords.partition; // 取数据以及更新本地 consume offset  List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt; records = drainRecords(nextInLineRecords, recordsRemaining); // 丢进 drained  ······ } } // 返回每个分区的数据  return drained; } 4. offset 提交 offset 提交有两种方式：同步和异步。提交者为 ConsumerCoordinator。提交 offset 前需保证是否存在活跃的连接以及谁是集群 Coordinator。主要方法为 sendOffsetCommitRequest。\nprivate RequestFuture\u0026lt;Void\u0026gt; sendOffsetCommitRequest(final Map\u0026lt;TopicPartition, OffsetAndMetadata\u0026gt; offsets) { // 检测 offsets 是否为空以及 coordinator 是否存在，如不存在，说明当前不处于有效分组内，不应该提交 offset.  ...... // create the offset commit request  Map\u0026lt;String, OffsetCommitRequestData.OffsetCommitRequestTopic\u0026gt; requestTopicDataMap = new HashMap\u0026lt;\u0026gt;(); for (Map.Entry\u0026lt;TopicPartition, OffsetAndMetadata\u0026gt; entry : offsets.entrySet()) { // 组装 OffsetCommitRequestTopic 和 分组  ······ } final Generation generation; // 获取当前消费者所属 group 的年代信息  if (subscriptions.partitionsAutoAssigned()) { generation = generation(); // 是否处于有效分组  if (generation == null) { log.info(\u0026#34;Failing OffsetCommit request since the consumer is not part of an active group\u0026#34;); return RequestFuture.failure(new CommitFailedException()); } } else // 由于 USER_ASSIGNED 类型不涉及 Rebalance, 故没有年代信息  generation = Generation.NO_GENERATION; // 组装请求，创建 OffsetCommitRequest 请求  OffsetCommitRequest.Builder builder = new OffsetCommitRequest.Builder( new OffsetCommitRequestData() .setGroupId(this.groupId) .setGenerationId(generation.generationId) .setMemberId(generation.memberId) .setGroupInstanceId(groupInstanceId.orElse(null)) .setTopics(new ArrayList\u0026lt;\u0026gt;(requestTopicDataMap.values())) ); return client.send(coordinator, builder) .compose(new OffsetCommitResponseHandler(offsets)); } 执行流程如下：\n 检测 offsets 是否为空，空则直接返回成功； 检测消费者所属 coordinator broker 实例的有效性，有效才可提交 offsets； 组装请求所需数据并分组； 获取年代信息，如果年代信息为 null, 则认为当前消费者并不属于此消费组，不应该提交 offsets； 组装发送 OffsetCommitRequest 请求并注册响应处理器。  当请求成功后，调用响应处理器 OffsetCommitResponseHandler 处理响应结果。\npublic void handle(OffsetCommitResponse commitResponse, RequestFuture\u0026lt;Void\u0026gt; future) { // 统计请求时长  sensors.commitLatency.record(response.requestLatencyMs()); Set\u0026lt;String\u0026gt; unauthorizedTopics = new HashSet\u0026lt;\u0026gt;(); // key 代表分区，value 代表错误码  for (Map.Entry\u0026lt;TopicPartition, Short\u0026gt; entry : commitResponse.responseData().entrySet()) { TopicPartition tp = entry.getKey(); OffsetAndMetadata offsetAndMetadata = this.offsets.get(tp); long offset = offsetAndMetadata.offset(); // 获取错误码对应的详细信息  Errors error = Errors.forCode(entry.getValue()); // 针对错误做特定处理：日志内容制定 是否需要才重试等等  if (error == Errors.NONE) { // 请求成功  log.debug(\u0026#34;Committed offset {} for partition {}\u0026#34;, offset, tp); } // 错误处理  ······ } if (!unauthorizedTopics.isEmpty()) { log.error(\u0026#34;Not authorized to commit to topics {} for group {}\u0026#34;, unauthorizedTopics, groupId); future.raise(new TopicAuthorizationException(unauthorizedTopics)); } else { future.complete(null); } } 同步和异步提交的区别只在与是否存在 ConsumerNetworkClient#poll 方法阻塞发送请求。\n5. Rebalance Rebalance 本质是一种协议，保证消费者数据消费的均衡。但在发生 Rebalance 时，消费组里面所有消费者都要停止工作，等待 Rebalance 完成。\n触发时机有3种：\n 组员个数发生变化； 订阅的 Topic 数量发生变化； 订阅的 Topic 的分区数量发生变化。  Rebalance 设计两步： Join 与 Sync：\n Join: 加入消费组。消费者发送 JoinGroupRequest 请求至集群 coordinator，请求加入消费组。coordinator 也在这个时候收集消费者信息并从中选择 consumer leader，并将组成员信息以及订阅信息交给 leader。 Sync: 同步分区分配信息。消费者发送 SyncGroupRequest 请求至集群 coordinator。其中 leader 将分区分配方案同步给 coordinator， coordinator 返回分配方案给消费者。  经过这两步后，消费关系已确定下来，接下来就是通过 Heartbeart 维持这段消费关系。如果 Heartbeart 没有按时发送，coordinator 将认定消费者 dead，将触发新一轮的 Rebalance。\n由于 Rebalance 使得消费组中的消费者停止工作，影响消费效率，故需尽可能避免不必要的 Rebalance。\n看看 HeartbeatThread 如何触发 Rebalance：\npublic void run() { try { log.debug(\u0026#34;Heartbeat thread started\u0026#34;); while (true) { synchronized (AbstractCoordinator.this) { ······ long now = time.milliseconds(); if (coordinatorUnknown()) { // 确认 coordinator 有效性  if (findCoordinatorFuture != null || lookupCoordinator().failed()) AbstractCoordinator.this.wait(retryBackoffMs); } else if (heartbeat.sessionTimeoutExpired(now)) { // 在 sessionTimeout 时间内，没有收到心跳请求的响应，需重新查找有效的 coordinator  markCoordinatorUnknown(); } else if (heartbeat.pollTimeoutExpired(now)) { // poll 调用之间超出 maxPollIntervalMs 时间，则认为 consumer 不可用，显示离开分组  log.warn(\u0026#34;This member will leave the group because consumer poll timeout has expired. This \u0026#34; + \u0026#34;means the time between subsequent calls to poll() was longer than the configured \u0026#34; + \u0026#34;max.poll.interval.ms, which typically implies that the poll loop is spending too \u0026#34; + \u0026#34;much time processing messages. You can address this either by increasing \u0026#34; + \u0026#34;max.poll.interval.ms or by reducing the maximum size of batches returned in poll() \u0026#34; + \u0026#34;with max.poll.records.\u0026#34;); maybeLeaveGroup(); } else if (!heartbeat.shouldHeartbeat(now)) { // 还不需要发送心跳，等待  AbstractCoordinator.this.wait(retryBackoffMs); } else { // 更新心跳, 并重置 heartbeatTimer  heartbeat.sentHeartbeat(now); // 发送 HeartbeatRequest 请求  sendHeartbeatRequest().addListener(new RequestFutureListener\u0026lt;Void\u0026gt;() { @Override public void onSuccess(Void value) { synchronized (AbstractCoordinator.this) { // 更新心跳，并重置 sessionTimer  heartbeat.receiveHeartbeat(); } } }); } } } } // 异常处理  ······ } 心跳涉及三个部分：\n heartbeatTimer. 下一次发送 HeartbeatRequest 请求的倒计时。超时后即发送请求。如果服务端在 sessionTimeout 时间内没收到请求，则认定 consumer dead，触发 Rebalance，其余消费者接收到心跳响应时，则开始 Join-Sync。 pollTimer. 下一次 poll 调用的倒计时。超时后 HeartbeatThread 将推送 LeaveGroupRequest 请求触发 Rebalance。 sessionTimer. 下一次收到心跳响应的倒计时。超时即表示没有收到服务端的心跳响应，consumer 认定 coordinator 不可靠，之后回在 poll 过程中确认 coordinator 是否有效，如果在 sessionTimeout 内还无法确认的话，则无法重置 pollTimer，则发生 2 情况。  故可知，想要避免不必要的 Rebalance，需要保证 poll 操作间隔时间不可以超出 maxPollIntervalMs，即消费数据的速度不能太慢，也可以通过 max.poll.records 调节拉取数量已达到效果。\n6. 参考  Kafka Rebalance机制分析 "});index.add({'id':9,'href':'/tags/netty/','title':"netty",'content':""});index.add({'id':10,'href':'/posts/netty-codec/','title':"Netty Codec",'content':"Codec是编解码器，用于数据格式的转换。在网络通信中，底层都是通过字节流交互。故在Netty中需要实现应用程序的数据与字节流的转换：当进行远程 跨进程服务调用时，需要将被传输的Java对象编码成字节数组或者ByteBuffer对象；当远程服务读取字节数组或者ByteBuffer对象，需要将其还原成发送时的对象。 这听起来十分熟悉，就跟数据库存取数据一般，需要保证取的结果跟存时的一样。\n Netty version: 4.1\n 1 Java序列化 通过上面的描述，感觉编解码跟Java序列化很相似。其实Java序列化也是编解码技术的一种实现。但是其有几个缺点：\n 无法跨语言 序列化后的码流大（空间） 性能低（时间）  但是Netty还是提供了Java序列化相应的Codec handler，可能因为方便使用吧。\n2 TCP粘包/拆包 TCP是一个面向字节流的协议，它是性质是流式的，所以它并没有分段，就像水流一样，你没法知道什么时候开始，什么时候结束。故有使用Netty进行TCP/IP网络通信会发生以下情况：\n UDP不会发生粘包\n 常见原因：\n 要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包 待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包  3 Netty如何实现编解码 通过Channel和Handler篇可知，编解码器通过Handler方式实现：\n 【Unsafe.read】-\u0026gt; Decoder handler 将ByteBuf解码为应用程序数据 -\u0026gt; 其他handler对解码后的数据进行处理 【Channel.write】-\u0026gt; 其他handler处理 -\u0026gt; Encoder handler 将对象编码为ByteBuf -\u0026gt;【Unsafe.write】   MessageToByteEncoder和ByteToMessageDecoder是ByteBuf和对象之间转换；MessageToMessageEncoder和MessageToMessageDecoder 是对象之间的转换。\n3.1 MessageToByteEncoder 可以猜想得到，对象肯定是通过ChannelOutboundHandler.write(...)方法的处理完成编码并将编码后的数据交给next handler\npublic void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { ByteBuf buf = null; try { // 可以通过自定义matcher匹配需要处理的消息  if (acceptOutboundMessage(msg)) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) I cast = (I) msg; // 分配buffer 用于存放编码后的数据。其使用的是初始化booststrap中设置的allocator:pool or unpool  buf = allocateBuffer(ctx, cast, preferDirect); try { // 需要子类实现的方法。编码协议，粘包/拆包处理等  encode(ctx, cast, buf); } finally { // 引用计数修改  ReferenceCountUtil.release(cast); } if (buf.isReadable()) { // 通知next handler。 tail-\u0026gt;head，直到head触发Unsafe.write  ctx.write(buf, promise); } else { buf.release(); ctx.write(Unpooled.EMPTY_BUFFER, promise); } buf = null; } else { ctx.write(msg, promise); } } catch (EncoderException e) { throw e; } catch (Throwable e) { throw new EncoderException(e); } finally { if (buf != null) { buf.release(); } } } 3.2 ByteToMessageDecoder ByteToMessageDecoder#channelRead(ChannelHandlerContext, Object)\nByteBuf data = (ByteBuf) msg; // cumulation 为上一次decode的消息，当其为null代表上一次消息已经完全解码，反之代表需要上一次的消息未完全解码 first = cumulation == null; if (first) { cumulation = data; } else { // 存在半包消息，ByteBuf累加  cumulation = cumulator.cumulate(ctx.alloc(), cumulation, data); } // 重头 callDecode(ctx, cumulation, out); cumulation是共享变量且没有同步操作，故decoder一定不可共享，否则会造成decode错误\nprotected void callDecode(ChannelHandlerContext ctx, ByteBuf in, List\u0026lt;Object\u0026gt; out) { try { while (in.isReadable()) { // 已解码的消息数量  int outSize = out.size(); if (outSize \u0026gt; 0) { fireChannelRead(ctx, out, outSize); out.clear(); // Check if this handler was removed before continuing with decoding.  // If it was removed, it is not safe to continue to operate on the buffer.  //  // See:  // - https://github.com/netty/netty/issues/4635  if (ctx.isRemoved()) { break; } outSize = 0; } int oldInputLength = in.readableBytes(); // decode message  decodeRemovalReentryProtection(ctx, in, out); // Check if this handler was removed before continuing the loop.  // If it was removed, it is not safe to continue to operate on the buffer.  //  // See https://github.com/netty/netty/issues/1664  if (ctx.isRemoved()) { break; } // out列表长度不变  if (outSize == out.size()) { // 未消费ByteBuf，为半包,  if (oldInputLength == in.readableBytes()) { break; } else { // 无效消息：太长或者其他原因，丢了不处理  continue; } } // 奇怪的东西：未更新readIndex，却解码了message，不合规范  if (oldInputLength == in.readableBytes()) { throw new DecoderException( StringUtil.simpleClassName(getClass()) + \u0026#34;.decode() did not read anything but decoded a message.\u0026#34;); } // 是否只解码一条消息  if (isSingleDecode()) { break; } } } catch (DecoderException e) { throw e; } catch (Exception cause) { throw new DecoderException(cause); } } 其子类通过继承这些类实现encode或者decode方法\n4 Netty中粘包/拆包的处理 LengthFieldBasedFrameDecoder为抽象类，ObjectDecoder是其子类，相应的编码其为ObjectEncoder\nObjectEncoder#encode(ChannelHandlerContext, Serializable, ByteBuf)\n由于其使用的Java自身的序列化技术，故消息必须实现Serializable\n其主要工作就是为Java序列化消息以及为编码后的消息增加消息头表示消息的长度，以便ObjectDecoder利用消息长度来解码消息\nObjectDecoder#decode(ChannelHandlerContext, ByteBuf)\nprotected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception { // 通过`LengthFieldBasedFrameDecoder#decode`获取实际的消息体  ByteBuf frame = (ByteBuf) super.decode(ctx, in); if (frame == null) { return null; } ObjectInputStream ois = new CompactObjectInputStream(new ByteBufInputStream(frame, true), classResolver); try { return ois.readObject(); } finally { ois.close(); } }if (discardingTooLongFrame) { // 消息过长，拆成多个packet过来。拆后的消息过长，丢弃  discardingTooLongFrame(in); } ... ... // 需要丢包 if (frameLength \u0026gt; maxFrameLength) { exceededFrameLength(in, frameLength); return null; } 相互呼应。当帧消息长度超过maxFrameLength，需要丢弃该消息，如果frameLength大于此刻帧消息长度，那应该是发送方发送消息时发生了拆包，那么下一次 收到的帧消息需要继续丢弃剩余长度。\nprivate void exceededFrameLength(ByteBuf in, long frameLength) { // 剩余需要丢弃的字节数。in.readableBytes()表示当前消息的消息长度  long discard = frameLength - in.readableBytes(); tooLongFrameLength = frameLength; if (discard \u0026lt; 0) { // buffer contains more bytes then the frameLength so we can discard all now  in.skipBytes((int) frameLength); } else { // Enter the discard mode and discard everything received so far.  discardingTooLongFrame = true; // 记下来 下一次继续丢  bytesToDiscard = discard; in.skipBytes(in.readableBytes()); } failIfNecessary(true); }// 通过协议定义的length file的偏移量以及其长度获取帧消息长度。 long frameLength = getUnadjustedFrameLength(in, actualLengthFieldOffset, lengthFieldLength, byteOrder);// 获取 int readerIndex = in.readerIndex(); int actualFrameLength = frameLengthInt - initialBytesToStrip; // 提取需解析的消息。使用ByteBuf#retainedSlice，零拷贝 ByteBuf frame = extractFrame(ctx, in, readerIndex, actualFrameLength); // 由于ByteBuf#retainedSlice不改变read/write index，需手动指定read index in.readerIndex(readerIndex + actualFrameLength); 其他几个也都是利用encode时为消息增加的额外字段处理粘包/拆包的：\n DelimiterBasedFrameDecoder是基于消息边界方式进行粘包拆包处理的 FixedLengthFrameDecoder是基于固定长度消息进行粘包拆包处理的 LineBasedFrameDecoder是基于行来进行消息粘包拆包处理的  link  TCP粘包拆包的产生原因分析及解决思路 什么是TCP粘包、为什么UDP不会粘包 "});index.add({'id':11,'href':'/posts/netty-handler/','title':"Netty Handler",'content':"Web服务器通常都会使用filter来处理请求，这样就将处理流程分离和解耦。在Netty中也一样，其拥有ChannelPipeline作为过滤器通道，消息在里面流动和传递，由其中的 Handler拦截处理。Handler之间又由Context（上下文）作为纽扣连接起来。\n Netty version: 4.1\n 1 pipeline事件传播机制 从源码上扣下来的：\n* I/O Request * via {@link Channel} or * {@link ChannelHandlerContext} * | * +---------------------------------------------------+---------------+ * | ChannelPipeline | | * | \\|/ | * | +---------------------+ +-----------+----------+ | * | | Inbound Handler N | | Outbound Handler 1 | | * | +----------+----------+ +-----------+----------+ | * | /|\\ | | * | | \\|/ | * | +----------+----------+ +-----------+----------+ | * | | Inbound Handler N-1 | | Outbound Handler 2 | | * | +----------+----------+ +-----------+----------+ | * | /|\\ . | * | . . | * | ChannelHandlerContext.fireIN_EVT() ChannelHandlerContext.OUT_EVT()| * | [ method call] [method call] | * | . . | * | . \\|/ | * | +----------+----------+ +-----------+----------+ | * | | Inbound Handler 2 | | Outbound Handler M-1 | | * | +----------+----------+ +-----------+----------+ | * | /|\\ | | * | | \\|/ | * | +----------+----------+ +-----------+----------+ | * | | Inbound Handler 1 | | Outbound Handler M | | * | +----------+----------+ +-----------+----------+ | * | /|\\ | | * +---------------+-----------------------------------+---------------+ * | \\|/ * +---------------+-----------------------------------+---------------+ * | | | | * | [ Socket.read() ] [ Socket.write() ] | * | | * | Netty Internal I/O Threads (Transport Implementation) | * +-------------------------------------------------------------------+ 可以发现：\n ChannelPipeline中的handler需要处理inbound和outbound两类事件。 inbound事件通常由I/O线程触发，如tcp链路建立事件，读事件等；outbound一般由用户主动发起的网络I/O请求，如用户发起连接，消息发送等操作。 inbound事件inbound事件由下至上处理由下至上处理：由底层触发生成inbound事件；outbound事件由上至下：收到用户操作生成的outbound事件。 事件通过ChannelHandlerContext在handler之间传递。  由Java Doc可知其一些特性：\n ChannelPipeline是thread-safe。pipeline在channel创建的时候生成并与channel绑定，每个channel都拥有自己的pipeline。注意：其中的ChannelHandler需要用户自行维护其线程安全。 ChannelPipeline可以动态添加或删除ChannelHander。比如当需要处理敏感信息时，添加加密Handler，处理完后再删除。 通常ChannelHandler都是由I/O线程处理，但是如果对于耗时操作可以通过额外的EventExecutorGroup来处理。  2 pipeline初始化 以DefaultChannelPipeline为例\nprotected DefaultChannelPipeline(Channel channel) { this.channel = ObjectUtil.checkNotNull(channel, \u0026#34;channel\u0026#34;); succeededFuture = new SucceededChannelFuture(channel, null); voidPromise = new VoidChannelPromise(channel, true); tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head; } Pipeline一般在Channel创建时生成。\n其中涉及了两个重要的角色：TailContext和HeadContext。它们构成了一条链表的首尾。\n3 pipeline添加handler public final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) { final AbstractChannelHandlerContext newCtx; synchronized (this) { // 检测channel的重复性： 当handler为sharable，允许被其他channel使用。  checkMultiplicity(handler); // 创建与handler相关的context。通过context实现消息在handlers之间流转  newCtx = newContext(group, filterName(name, handler), handler); // 添加到处理链。addLast，不是让其成为tail，而是tail的pre节点。  addLast0(newCtx); // If the registered is false it means that the channel was not registered on an eventLoop yet.  // In this case we add the context to the pipeline and add a task that will call  // ChannelHandler.handlerAdded(...) once the channel is registered.  // 需要在channel注册成功后，触发handlerAdded事件。  if (!registered) { // 如果未注册成功，标记context尚未/即将调用handlerAdded方法。  newCtx.setAddPending(); // 添加待执行任务。等到合适的时候再触发  callHandlerCallbackLater(newCtx, true); return this; } // 如果未设置额外的EventExecutorGroup，则使用reactor线程，即NioEventLoop，否则使用设置的EventExecutorGroup中的线程  // 这样做的好处：事件处理都由统一的线程完成，避免并发问题  EventExecutor executor = newCtx.executor(); if (!executor.inEventLoop()) { callHandlerAddedInEventLoop(newCtx, executor); return this; } } callHandlerAdded0(newCtx); return this; } pipeline有很多操作handler的方法，方法内容大同小异：\n synchronized内置锁来一波，防止出现并发问题。因为channel可能会被用户线程并发使用(push) add/remove，context链调整 当channeladd/remove成功后触发Handler add/remove事件  其中添加task至pendingHandlerCallbackHead为了延迟调用直到channel注册成功，为什么呢？\n看看这个task做了什么，以PendingHandlerAddedTask为例：\n DefaultChannelPipeline.callHandlerAdded0(\u0026hellip;)-\u0026gt;AbstractChannelHandlerContext.callHandlerAdded()\n final void callHandlerAdded() throws Exception { // We must call setAddComplete before calling handlerAdded. Otherwise if the handlerAdded method generates  // any pipeline events ctx.handler() will miss them because the state will not allow it.  // ，不然如果handerAdded方法中触发i/o事件如read/write等，当前context会因为  // 状态不允许\u0026lt;code\u0026gt;invokeHandler()\u0026lt;code\u0026gt;，无法处理此时产生的事件  if (setAddComplete()) { handler().handlerAdded(this); } } 如注释描述：必须要在调用handlerAdded方法前调用setAddComplete完成状态设置。否则此时调用后，Handler#handlerAdded方法里生成了事件，此handler因为状态不正确无法处理，导致事件丢失。\n因为Pipeline是将事件交由Context处理，基本类似以下伪代码：\nprivate void dealEvent(Object event) { if (invokeHandler()) { 当前context处理event并传递 } else { 跳过当前context并传递 } } 其中invokeHandler()方法，判断当前是否可以调用handler处理event\nprivate boolean invokeHandler() { // Store in local variable to reduce volatile reads.  int handlerState = this.handlerState; // If not return {@code false} and if called or could not detect return {@code true}.  return handlerState == ADD_COMPLETE || (!ordered \u0026amp;\u0026amp; handlerState == ADD_PENDING); } 看其中的判定条件handlerState == ADD_COMPLETE，通过setAddComplete可以达到条件。一言蔽之，需要channel注册后才能处理事件，否则跳过。\n!ordered \u0026amp;\u0026amp; handlerState == ADD_PENDING是指Handler拥有的EventExecutor不是有序执行task且ADD_PENDIN。这种情况一般是针对于非NIO的情况\n但是在NIO情况下，如果为handler设置的额外EventExecutor不是task执行不是有序的，可能还是有问题？\n 4 pipeline api pipeline是事件传播的入口。\npublic interface ChannelPipeline extends ChannelInboundInvoker, ChannelOutboundInvoker, Iterable\u0026lt;Entry\u0026lt;String, ChannelHandler\u0026gt;\u0026gt;\n有两类事件传播的方法\n inbound事件传播方法：fireChannelRead fireChannelReadComplete fireChannelActive等fireXX，处理由I/O线程触发的事件，用于将inbound event交给head\n outbound事件传播方法：bind connect read write等网络I/O操作，处理由系统外部I/O操作触发的请求，用于将outbound event交给tail\n  以DefaultChannelPipeline.fireChannelRegistered()为例介绍inbound事件传播流程\n// DefaultChannelPipeline public final ChannelPipeline fireChannelRegistered() { //\tinbound事件传播的入口，head(HeadContext)-\u0026gt;tail(TailContext)  AbstractChannelHandlerContext.invokeChannelRegistered(head); return this; } fireChannelRegistered()方法将ChannelRegistered事件交给head，开始传播\n// AbstractChannelHandlerContext static void invokeChannelRegistered(final AbstractChannelHandlerContext next) { // 获取执行handler的EventExecutor.当没额外设置则使用Channel所绑定的EventExecutor  EventExecutor executor = next.executor(); // 当前线程是否属于EventExecutor的线程  // task由统一的EventExecutor执行，避免并发问题  if (executor.inEventLoop()) { next.invokeChannelRegistered(); } else { // 提交任务  executor.execute(new Runnable() { @Override public void run() { next.invokeChannelRegistered(); } }); } } private void invokeChannelRegistered() { // 判断当前context是否可以处理事件  if (invokeHandler()) { try { // 由HeadContext-\u0026gt;TailContext 。在`Handler#channelRegistered`方法中完成自定义处理后，又调用`fireChannelRegistered`方法传播给下一个context  ((ChannelInboundHandler) handler()).channelRegistered(this); } catch (Throwable t) { notifyHandlerException(t); } } else { // 否则跳过，找下一个可用context  fireChannelRegistered(); } } public ChannelHandlerContext fireChannelRegistered() { invokeChannelRegistered(findContextInbound()); return this; } 再看看其他处理inbound事件的方法，大同小异：\n Pipeline调用fireXX方法将inbound事件传给HeadContext进行传播 获取当前context所绑定的EventExecutor，由统一的线程处理事件 处理完后寻找下一个为inbound的context 继续2～3操作，直到tail(TailContext)  outbound事件传播跟inbound十分类似：\n Pipeline调用fireXX方法将inbound事件传给HeadContext进行传播 获取当前context所绑定的EventExecutor，由统一的线程处理事件 处理完后寻找下一个为inbound的context 继续2～3操作，直到tail(TailContext)  5 TailContext 和 HeadContext final class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler\nfinal class HeadContext extends AbstractChannelHandlerContext implements ChannelOutboundHandler, ChannelInboundHandler\n类关系如上，TailContext和HeadContext既是一个context，也是一个handler。它们组成的双向链表如下：\n解释下该图：\n inbound事件从HeadContext开始，寻找链表中inbound context直至TailContext outbound事件从TailContext开始，寻找链表中的oubound context直至HeadContext  初始化如下：\nHeadContext(DefaultChannelPipeline pipeline) { // 默认没有childExecutor  super(pipeline, null, HEAD_NAME, true, true); // i/o操作的执行入口  unsafe = pipeline.channel().unsafe(); // 设置handlerState状态为complete  setAddComplete(); } TailContext(DefaultChannelPipeline pipeline) { super(pipeline, null, TAIL_NAME, true, false); setAddComplete(); } HeadContext既是InBoundHandler和OutBoundHandler ；TailContext仅为InBoundHandler。InBoundHandler专处理inbound event; OutBoundHandler专处理outbound event\nHeadContext的网络I/O方法都是使用Unsafe来实现的。其接近底层\nTailContext的Fire Event方法基本上没有什么操作，只是收尾：回收消息等\n6 ServerBootstrapAcceptor server channel通过它将client channel分配给works(child evenLoopGroup)，自行追踪源码：\n NioEventLoop.run()-\u0026gt;NioMessageUnsafe.read()-\u0026gt;DefaultChannelPipeline.fireChannelRead(Object)-\u0026gt;HeadContext.channelRead(\u0026hellip;)-\u0026gt;\u0026hellip;-\u0026gt; ServerBootstrapAcceptor.channelRead(\u0026hellip;)-\u0026gt;TailContext.channelRead(\u0026hellip;)\n 7 Encoder和Decoder 网络中都是通过字节码的形式来传输数据的。在JAVA程序中，客户端发送Request需要将其转换成字节码传输，服务端接收Request将这些字节码还原成对象。其中 对象转换成字节码的过程称为编码（Encode），字节码还原成对象的过程称为解码（Decode）。\nNetty是一个网络I/O框架，那肯定也包含了编解的操作，这些操作包装为handler。有以下类型：\n   Type Function     ByteToMessageDecoder 将ByteBuf解码成POJO   MessageToMessageDecoder 将POJO解码成POJO   MessageToByteEncoder 将POJO编码成ByteBuf   MessageToMessageEncoder 将POJO编码成POJO   LengthFieldBasedFrameDecoder 半包解码器   LengthFieldPrepender 半包编码器    Encoder为outbound方法write传播。一般紧随head节点之后，将转换的信息传给head，让其通过Unsafe中的write方法将信息发送出去\nDecoder为inbound方法fireChannelRead触发。一般紧随head节点之后，将NioUnsafe.read读取的信息转换并传递下去\nDecoder=Encoder=TailContext-- 后续详解：粘包和拆包，高效的序列化协议（Protobuf，Thrift等）\n8 小结  Channel为数据传输的通道，Pipeline为Channel次级处理通道，其中的handlers是事件处理单元 在Pipeline中存在两种消息：inbound和outbound。inbound事件一般指某，outbound事件一般指网络I/O操作。 Pipeline中有一条context链，将多个Handler关联起来，形成了一个事件传播链。其中必然存在head(HeadContext)以及tail(TailContext)。inbound流向：head-\u0026gt;tail；outbound流向：tail-\u0026gt;head  link  SOFA 源码分析 —— 过滤器设计 netty源码分析之pipeline(二) Netty-理解Pipeline "});index.add({'id':12,'href':'/posts/netty-channel/','title':"Netty Channel",'content':"channel在编程中是一个很常见的东西，中文解释为：通道。其一般是数据传输的媒介，例如FileChannel，SocketChannel等，可以进行异步的I/O操作。 在Netty中也不例外，那么来看看其跟Java Nio Channel是如何配合的。\n0 目录  简介 Channel Unsafe   Netty version: 4.1\n 1 简介 Netty Channel是网络I/O读写抽象出的接口，其只是Java Nio Channel的一个门面，为众多Nio Channel功能扩展提供了统一的接口。\nwhy they try to reinvent the wheel? 那肯定是轮子不合适啊！《Netty权威指南》有解释，这里简单概括：\n 使用不方便：原生channel接口不统一，切换不爽。 扩展不方便：不能很好与Netty的架构融合。  自定义后的Channel除了拥有基础的网络I/O功能，兼容多个I/O模型：NIO/OIO，提供了统一视图。另外，可以更加细致地配置网络I/O。Netty是一个 事件驱动的框架，则Channel也实现网络I/O事件通知以便用户通过handler灵活处理各个事件，类似AOP的功能。\n 2 Channel 2.1 主要api 除了网络I/O的必要操作：bind，connect，read，write等相关操作。另外还有Netty的特色方法：\n ChannelId id()。获取当前channel的唯一标识。 EventLoop eventLoop()。获取当前channel所注册的eventLoop。每个channel只能注册一个eventLoop，一个eventLoop可以绑定多个channel，eventLoop通过多路复用器触发channel的I/O操作。 Channel parent()。对于服务端channel，返回null；对于客户端channel，则返回服务端parent eventLoop中接收连接的channel。注意， 这里说的服务端channel不是指服务端产生的channel，而是指ServerSocketChannel，客户端Channel也是指SocketChannel。具体后面有介绍。 ChannelMetadata metadata()。获取channel配置，网络I/O配置等。  Unsafe unsafe()。I/O操作工具类。 ChannelPipeline pipeline()。pipeline，一个handler\u0026rdquo;管道\u0026rdquo;，类似拦截器的功能，拦截处理channel的事件。可以利用其进行I/O操作。  channel的子类很多，就看其中两个常见的 NioServerSocketChannel 和 NioSocketChannel以了解channel在netty中是怎么工作的。\n从相同的地方开始说起\n2.2 AbstractChannel Channel的基本骨架，实现了其大部分功能。主要看看I/O操作。\npublic ChannelFuture connect(SocketAddress remoteAddress) { return pipeline.connect(remoteAddress); } public ChannelFuture connect(SocketAddress remoteAddress) { return pipeline.connect(remoteAddress); } // read, write等 .... .... 发现I/O操作都有pipeline的身影。pipeline是channel内部更细的一个通道，更加细致的处理I/O事件。 其使用了责任链模式，事件在pipeline里面传播并由相应handler处理。下篇再详细介绍。\n2.3 AbstractNioChannel 从私有变量看起，这可是这个类独有的东西，或许可以看出一个类的大概功能。\n关注下readInterestOp属性，用于设置selector感兴趣的option。取值于NIO.SelectionKey中的OP_READ，OP_WRITE，OP_CONNECT，OP_ACCEPT。 Nio client channel是OP_READ，偏向字节流消息的处理；Nio server channel是OP_ACCEPT，偏向连接等对象的处理。\nselectionKey是channel注册到eventLoop后返回的选择键，便于调整interestOps。另外其是volatile，也没有CAS更新操作，仅保证内存可见性，保证写互斥。说明其可能会被多个线程同时使用。\nreadPending，字面意思 \u0026ldquo;读等待中\u0026rdquo;，主要为了判断是否能清除op_read。\nconnectPromise表示连接结果；connectTimeoutFuture表示定时检测连接是否超时的结果。\n通过这些，感觉该类偏向维护网络通道状态以及设置网络操作位。\n2.3.1 doRegister() 注册nio.channel\nprotected void doRegister() throws Exception { ... ... // 使用了nio.channel的原生方法，将channel注册到selector并初始化interest ops为0，表示不处理任何事件  selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); ... ... } 2.3.2 doBeginRead() 读操作之前的准备，该方法一般是在连接建立（channelActive）后执行\nprotected void doBeginRead() throws Exception { // Channel.read() or ChannelHandlerContext.read() was called  final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) { return; } // 标志此时有读操作正在处理  readPending = true; final int interestOps = selectionKey.interestOps(); // 设置通道的网络操作位以监听网络的读事件  if ((interestOps \u0026amp; readInterestOp) == 0) { selectionKey.interestOps(interestOps | readInterestOp); } } 2.4 AbstractNioMessageChannel inputShutdown标识input数据流是否关闭\n2.4.1 doWrite(ChannelOutboundBuffer) 在一个for循环中进行操作（doWriteMessage），直至message处理完成。注意，完全发送完后需要将OP_WRITE从通道的网络操作位中删除，防止重复触发。OP_WRITE可以看作是一次性事件，只有在socket发送缓冲区不足时才会设置。\n2.5 NioServerSocketChannel 服务端channel，故 doDisconnect()和doWriteMessage(Object, ChannelOutboundBuffer)等属于客户端channel的方法都不支持。\nMETADATA定义了channel的属性；DEFAULT_SELECTOR_PROVIDER为系统级别的selector provider。\nnewSocket(SelectorProvider)方法则创建nio中ServerSocketChannel。\n2.5.1 NioServerSocketChannel(ServerSocketChannel) NioServerSocketChannel的初始化方法\npublic NioServerSocketChannel(ServerSocketChannel channel) { // 将 `readInterestOp`设置为OP_ACCEPT， parent为null，表明其偏向接收客户端连接  super(null, channel, SelectionKey.OP_ACCEPT); config = new NioServerSocketChannelConfig(this, javaChannel().socket()); } 2.5.2 doBind(SocketAddress) 将channel的socket与本地地址绑定并监听\nprotected void doBind(SocketAddress localAddress) throws Exception { if (PlatformDependent.javaVersion() \u0026gt;= 7) { javaChannel().bind(localAddress, config.getBacklog()); } else { javaChannel().socket().bind(localAddress, config.getBacklog()); } } back_log是tcp的配置，指定连接队列的大小。类似一个消息队列，请求顺序被执行。\n2.5.3 doReadMessages(List\u0026lt; Object \u0026gt;) 对于服务端channel，其读操作就是接收客户端连接\nprotected int doReadMessages(List\u0026lt;Object\u0026gt; buf) throws Exception { // 接收连接  SocketChannel ch = SocketUtils.accept(javaChannel()); try { if (ch != null) { // 封装为客户端channel并保存  buf.add(new NioSocketChannel(this, ch)); return 1; } } catch (Throwable t) { ... ... } return 0; } 2.6 AbstractNioByteChannel 类名跟AbstractNioMessageChannel很相似，只是一个是Message，一个是Byte。通过其各自的doReadXX方法和doWriteXX方法的参数可以得知，这是指其消息类型的区别，AbstractNioMessageChannel处理的消息为对象， AbstractNioByteChannel处理的消息为字节流。\n2.6.1 AbstractNioByteChannel(Channel, SelectableChannel) protected AbstractNioByteChannel(Channel parent, SelectableChannel ch) { // 将 `readInterestOp`设置为OP_READ，表明其偏向接收客户端消息  super(parent, ch, SelectionKey.OP_READ); } 2.6.2 doWriteInternal(ChannelOutboundBuffer, Object) // 返回值代表读取的消息数 private int doWriteInternal(ChannelOutboundBuffer in, Object msg) throws Exception { if (msg instanceof ByteBuf) { ByteBuf buf = (ByteBuf) msg; // 是否已经读完，是则删除消息  if (!buf.isReadable()) { in.remove(); return 0; } // localFlushedAmount是write至缓冲区的字节数，用于记数  final int localFlushedAmount = doWriteBytes(buf); // 如果localFlushedAmount\u0026lt;=0，则当前缓冲区空间不足  if (localFlushedAmount \u0026gt; 0) { in.progress(localFlushedAmount); if (!buf.isReadable()) { in.remove(); } return 1; } } ... ... // FileRegion的处理，大同小异  return WRITE_STATUS_SNDBUF_FULL; } 2.6.3 doWrite(ChannelOutboundBuffer) super.flush0-\u0026gt;doWrite，直至消息完全写完或者TCP socket发送缓冲区不足\nprotected void doWrite(ChannelOutboundBuffer in) throws Exception { // 获取write次数  int writeSpinCount = config().getWriteSpinCount(); // 循环发送消息（FileRegion or ByteBuf），可能一次发送不完。这种不完全发送的消息简称半包  do { Object msg = in.current(); if (msg == null) { // 消息已经完全写完，清除OP_WRITE  clearOpWrite(); return; } writeSpinCount -= doWriteInternal(in, msg); } while (writeSpinCount \u0026gt; 0); // writeSpinCount\u0026gt;0: 消息已经完全写完  // writeSpinCount==0: 异步继续write-flush剩余半包  // writeSpinCount\u0026lt;0: 发送缓冲区不足，设置OP_WRITE  incompleteWrite(writeSpinCount \u0026lt; 0); } 2.7 NioSocketChannel 客户端channel\n2.7.1 doConnect(SocketAddress, SocketAddress) 建立连接\nprotected boolean doConnect(SocketAddress remoteAddress, SocketAddress localAddress) throws Exception { if (localAddress != null) { // 绑定本地地址  doBind0(localAddress); } boolean success = false; try { // false: 没抛异常，只是暂时连接不上，注册OP_CONNECT，等待connect ready  // true: 连接成功  boolean connected = SocketUtils.connect(javaChannel(), remoteAddress); if (!connected) { selectionKey().interestOps(SelectionKey.OP_CONNECT); } success = true; return connected; } finally { // I/O异常，说明客户端的TCP握手请求直接被reset或者被拒绝，关闭channel。  if (!success) { doClose(); } } } 2.7.2 doWrite(ChannelOutboundBuffer) 覆写 AbstractNioByteChannel的doWrite方法\nprotected void doWrite(ChannelOutboundBuffer in) throws Exception { SocketChannel ch = javaChannel(); int writeSpinCount = config().getWriteSpinCount(); do { // 没有消息可写了，清除标识  if (in.isEmpty()) { clearOpWrite(); return; } // Ensure the pending writes are made of ByteBufs only.  // maxBytesPerGatheringWrite为user发送缓冲区的大小  int maxBytesPerGatheringWrite = ((NioSocketChannelConfig) config).getMaxBytesPerGatheringWrite(); // 获取消息由ByteBuf组成的ByteBuffers.  ByteBuffer[] nioBuffers = in.nioBuffers(1024, maxBytesPerGatheringWrite); // 获取数组数量，但还不清楚为什么直接nioBuffers.length。其值大于等于0  int nioBufferCnt = in.nioBufferCount(); // Always us nioBuffers() to workaround data-corruption.  // See https://github.com/netty/netty/issues/2761  switch (nioBufferCnt) { case 0: // 0的可能性：可能消息类型为FileRegion or 消息尚未flush。doWrite0可以处理FileRegion  // We have something else beside ByteBuffers to write so fallback to normal writes.  writeSpinCount -= doWrite0(in); break; case 1: { ByteBuffer buffer = nioBuffers[0]; int attemptedBytes = buffer.remaining(); final int localWrittenBytes = ch.write(buffer); // 当可用缓冲区不足时注册OP_WRITE  if (localWrittenBytes \u0026lt;= 0) { incompleteWrite(true); return; } // 调整user发送缓冲区的大小。因为有些系统可能会动态调整SO_SNDBUF（os发送缓冲区）  adjustMaxBytesPerGatheringWrite(attemptedBytes, localWrittenBytes, maxBytesPerGatheringWrite); // 删除已完全发送的消息，释放资源  in.removeBytes(localWrittenBytes); --writeSpinCount; break; } default: { // 大于1: 看起来来和等于1的情况很相似，主要不同的地方就是获取attemptedBytes，不太懂  long attemptedBytes = in.nioBufferSize(); final long localWrittenBytes = ch.write(nioBuffers, 0, nioBufferCnt); if (localWrittenBytes \u0026lt;= 0) { incompleteWrite(true); return; } // Casting to int is safe because we limit the total amount of data in the nioBuffers to int above.  adjustMaxBytesPerGatheringWrite((int) attemptedBytes, (int) localWrittenBytes, maxBytesPerGatheringWrite); in.removeBytes(localWrittenBytes); --writeSpinCount; break; } } } while (writeSpinCount \u0026gt; 0); // 到这边的都是没写完的：1. 缓冲区满了 2.writeSpinCount用尽  incompleteWrite(writeSpinCount \u0026lt; 0); } writeSpinCount存在的意义：控制发送上限。以免数据过大，发送缓冲区满时导致reactor线程（I/O线程）长时间白白等待。\n2.7.3 ChannelOutboundBuffer.removeBytes(long) // writtenBytes：已发送的字节数 public void removeBytes(long writtenBytes) { for (;;) { // 当前发送的消息  Object msg = current(); if (!(msg instanceof ByteBuf)) { assert writtenBytes == 0; break; } final ByteBuf buf = (ByteBuf) msg; // 当前read的位置  final int readerIndex = buf.readerIndex(); // 待发送的字节数  final int readableBytes = buf.writerIndex() - readerIndex; // 完全发送消息A  if (readableBytes \u0026lt;= writtenBytes) { // 半包消息B  if (writtenBytes != 0) { // 更新发送进度  progress(readableBytes); // 获取半包消息B已发送的字节数  writtenBytes -= readableBytes; } // 删除已经发送的ByteBuf,释放资源  remove(); } else { // readableBytes \u0026gt; writtenBytes 即半包发送  if (writtenBytes != 0) { // 更新readerIndex，以防重复读写  buf.readerIndex(readerIndex + (int) writtenBytes); progress(writtenBytes); } break; } } clearNioBuffers(); } 3 Unsafe Unsafe接口是Channel专属的辅助接口。\u0026rdquo;Unsafe\u0026rdquo;表明其最好不要由用户调用，除非很熟netty。\nI/O读写操作实际都是由Unsafe接口完成的。\n3.1 api  void register(EventLoop, ChannelPromise)。将channel注册到selector多路复用器上，完成后通知ChannelFuture。 void bind(SocketAddress, ChannelPromise)。绑定指定的本地地址到channel上，完成后通知ChannelFuture。 void connect(SocketAddress, SocketAddress, ChannelPromise)。bind完成后，连接服务端，完成后通知ChannelFuture。 void beginRead()。设置网络操作用于读取消息。 void write(Object, ChannelPromise)。通过channel写消息，完成后通知ChannelFuture。 void flush()。将发送缓冲数组中的消息写入channel。  3.2 AbstractUnsafe outboundBuffer存储待write的消息。recvHandle记录read情况。inFlush0标识当前是否正在flush。\n3.2.1 register(EventLoop, final ChannelPromise) public final void register(EventLoop eventLoop, final ChannelPromise promise) { ... ... if (eventLoop.inEventLoop()) { register0(promise); } else { eventLoop.execute(new Runnable() { public void run() { register0(promise); } }); } ... ... } if (eventLoop.inEventLoop()) doA else eventLoop.execute(doA) 这种模式很常见。 如果当前线程不是当前channel所绑定eventLoop的reactor线程，则加入任务队列等待reactor线程执行，这样避免了多线程并发操作导致线程安全问题。\n主要操作为register0：\nprivate void register0(ChannelPromise promise) { try { // check if the channel is still open as it could be closed in the mean time when the register  // call was outside of the eventLoop  if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } boolean firstRegistration = neverRegistered; // 抽象方法。channel扩展实现，在channel与eventLoop绑定后调用，将channel注册到selector多路复用器上  doRegister(); neverRegistered = false; registered = true; // 在bind或者connect返回promise之前add handlers to channel pipeline.  pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); // ChannelRegistered：channel注册到selector后触发，感觉跟方法注解描述不一致  pipeline.fireChannelRegistered(); // Only fire a channelActive if the channel has never been registered. This prevents firing  // multiple channel actives if the channel is deregistered and re-registered.  // 判断是否connect成功。一般来说调用connect()方法后，先register后connect，此时是不可能连接成功的。  // 只有在channel调用deregistered后有重新registere，导致重新绑定了一个新的evenLoop，以免再次触发ChannelActive。  if (isActive()) { if (firstRegistration) { pipeline.fireChannelActive(); } else if (config().isAutoRead()) { // This channel was registered before and autoRead() is set. This means we need to begin read  // again so that we process inbound data.  //  // See https://github.com/netty/netty/issues/4805  // 重新register后设置网络操作用于读取消息  beginRead(); } } } catch (Throwable t) { // Close the channel directly to avoid FD leak.  closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } 3.2.2 write(Object, ChannelPromise) public final void write(Object msg, ChannelPromise promise) { assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) { // If the outboundBuffer is null we know the channel was closed and so  // need to fail the future right away. If it is not null the handling of the rest  // will be done in flush0()  // See https://github.com/netty/netty/issues/2362  safeSetFailure(promise, WRITE_CLOSED_CHANNEL_EXCEPTION); // release message now to prevent resource-leak  ReferenceCountUtil.release(msg); return; } int size; try { // 转换msg。 nio模式：将ByteBuffer转成directBuffer  msg = filterOutboundMessage(msg); // 获取消息字节数  size = pipeline.estimatorHandle().size(msg); if (size \u0026lt; 0) { size = 0; } } catch (Throwable t) { safeSetFailure(promise, t); ReferenceCountUtil.release(msg); return; } // 将消息缓存起来，此时还未发送，需等flush操作  outboundBuffer.addMessage(msg, size, promise); } 3.2.3 flush() public final void flush() { ... ... // 当前缓存的消息状态设置为flushed以便后面flush0()发送  outboundBuffer.addFlush(); flush0(); } flush0():\nprotected void flush0() { if (inFlush0) { // 防重入  return; } ... ... inFlush0 = true; ... ... try { // 将数据写入channel，进行网络传输，由具体的channel实现。  doWrite(outboundBuffer); } catch (Throwable t) { ... ... } finally { inFlush0 = false; } }  ps： nio read/write都会使用将数据转换direct buffer。详情\n 3.3 AbstractNioUnsafe 3.3.1 connect(final SocketAddress, final SocketAddress, final ChannelPromise) public final void connect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) { if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } try { if (connectPromise != null) { // Already a connect in process.  throw new ConnectionPendingException(); } boolean wasActive = isActive(); // channel扩展实现，nio.connect  if (doConnect(remoteAddress, localAddress)) { // 立即返回成功，触发第一次的ChannelActive  fulfillConnectPromise(promise, wasActive); } else { // false 不代表失败，可能需要等待，直接抛异常才算失败。  connectPromise = promise; requestedRemoteAddress = remoteAddress; // 注册connecttimeout定时任务。connect time out 关闭channel以及释放其他相关资源  int connectTimeoutMillis = config().getConnectTimeoutMillis(); if (connectTimeoutMillis \u0026gt; 0) { connectTimeoutFuture = eventLoop().schedule(new Runnable() { @Override public void run() { ChannelPromise connectPromise = AbstractNioChannel.this.connectPromise; ConnectTimeoutException cause = new ConnectTimeoutException(\u0026#34;connection timed out: \u0026#34; + remoteAddress); if (connectPromise != null \u0026amp;\u0026amp; connectPromise.tryFailure(cause)) { close(voidPromise()); } } }, connectTimeoutMillis, TimeUnit.MILLISECONDS); } // 为connect设置回调，处理connect cancel的情况  promise.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (future.isCancelled()) { if (connectTimeoutFuture != null) { connectTimeoutFuture.cancel(false); } connectPromise = null; close(voidPromise()); } } }); } } catch (Throwable t) { promise.tryFailure(annotateConnectException(t, remoteAddress)); closeIfClosed(); } } 3.4 NioByteUnsafe 3.4.1 read() 读取字节流\npublic final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); // 获取byteBuf分配器  final ByteBufAllocator allocator = config.getAllocator(); // RecvByteBufAllocator的用途是分配一个容量不大不小接收buffer，通过Handle扩展实现  final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { // 分配buffer，主要由于两种方式： fixed和adaptive，具体区别主要在guess()，record方法  // adaptive：record方法，根据此次读取的字节数预判调整下一次buffer的大小。查看record方法发现：  // 缩小容量需要连续两次低于当前容量，增加只要一次。  byteBuf = allocHandle.allocate(allocator); // 记录最新的读取，方便调整buffer以及判断读取是否结束  allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() \u0026lt;= 0) { // nothing was read. release the buffer.  byteBuf.release(); byteBuf = null; close = allocHandle.lastBytesRead() \u0026lt; 0; if (close) { // There is nothing left to read as we received an EOF.  readPending = false; } break; } allocHandle.incMessagesRead(1); // 这个标志主要是为了清除op_read存在的。防止read请求丢失  readPending = false; // read一次消息触发一次ChannelRead，消息有可能拆/粘包了，首先需要触发decode?  pipeline.fireChannelRead(byteBuf); // 释放资源  byteBuf = null; // 判断是否需要继续read  } while (allocHandle.continueReading()); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); if (close) { closeOnRead(pipeline); } } catch (Throwable t) { handleReadException(pipeline, byteBuf, t, close, allocHandle); } finally { // 注释说的很清楚了，当AutoRead为false时，想要清除op_read，需检测属否有正在等待的读，有两种情况：  // Check if there is a readPending which was not processed yet.  // This could be for two reasons:  // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method  // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method  //  // 如果不这样做，会导致等待的读无法完成，即Channel.read()无效。  // See https://github.com/netty/netty/issues/2254  if (!readPending \u0026amp;\u0026amp; !config.isAutoRead()) { removeReadOp(); } } } 3.5 NioMessageUnsafe 3.5.1 read() 与 NioByteUnsafe.read() 大同小异，具体在于读取的消息。doReadMessages(List \u0026lt;Object\u0026gt;)channel扩展，读取Object，有可能是channel，也有可能是packet。\n4 小结 channel总体可以划分成server和client两类。其实现了多种传输层协议，如TCP，UDP，SCTP等，多个I/O模型，如NIO，BIO等。\nserver channel一般只是accept连接，生成client channel，由client channel处理消息。\n在以上的源码中，我主要对其I/O操作进行了简单解析。可以发现，Channel实际上是调用内部聚合的Unsafe中的方法实现网络I/O，Channel只是为真正的I/O做好准备以及提供事件扩展。\n"});index.add({'id':13,'href':'/posts/netty-eventloop/','title':"Netty EventLoop",'content':"Netty的NIO线程模型为Reactor，而EventLoop是Reactor的实现部分。\n Netty version: 4.1\n NIO 一般来说，一次请求的具体过程为：用户进程发起请求，内核接受到请求后，从I/O设备中获取数据到buffer中，再将buffer中的数据copy到用户进程的地址空间，该用户进程获取到数据后再响应客户端。\n对于BIO，per connection per thread，I/O操作（Read,Write,Accept等）是阻塞的，因为内核空间和用户空间不能直接操作，数据需要进行拷贝，导致这部分的阻塞时间内线程白白阻塞，这样就成为高并发量的瓶颈。 I/O操作的阻塞时间主要由等待事件（ready）和内存拷贝（copy）两部分组成。而内存拷贝一般可以忽略不计。故主要阻塞在于等待时间花费的时间（Ready）。\nnio的出现主要是为了提高并发量，并不会提高响应的速度。其大多使用I/O多路复用模式（Reactor），使用一个线程轮询已经准备好的事件然后通知用户线程去处理，避免所有的线程都阻塞，节约了线程资源。\n PS: 使用NIO != 高性能，当连接数\u0026lt;1000，并发程度不高或者局域网环境下NIO并没有显著的性能优势。\n Reactor模式 简单来说，有3中模式：\n 单线程Reactor  接收连接和处理请求都在一个Reactor线程中完成。\n 多线程Reactor  将接收和处理分离，一个线程接收连接，一组线程处理请求。瓶颈在于接收线程。\n 主从多线程Reactor  一组线程接收连接，一组线程处理请求。\n一般实现有 selector 和 epoll。\n If you are running on linux you can use EpollEventLoopGroup and so get better performance, less GC and have more advanced features that are only available on linux.\n EventLoop 对Netty来说，Reactor的一般实现为NioEventLoop和EpollEventLoop。 EpollEventLoop受系统限制：epoll仅工作于Linux；NioEventLoop则对应seletor。\n Netty支持单线程、多线程模型、主从多线程模型\n 在我的认知中，Client的模型一般是单线程Reactor模式；Server的模型一般是多线程模式。 虽然ServerBootstrap有两个group（类似ThreadPool的概念），看起来像主从Reactor，但是根据源码，但server初始化bind端口，只会从group中使用一个eventloop接收连接。除非多次bind。\nparent group的多线程的真正意义?\n当一个JVM有两个不同的ServerBootstrap，共享一个parent group。如果parent group中只有一个线程，极端情况： boss线程一直只处理一个ServerBootstrap的连接。 so\u0026hellip;\u0026hellip; (当然你也可以强行认为即便多线程也全部只处理一个啦)。总之，这种情况可以提高效率。\n当acceptor线程接收到连接，创建channel并绑定到child group中的某个eventloop，I/O操作（Read,Write）以及业务处理都有这个eventloop完成，这要就实现了局部无锁化，避免了很多多线程竞争的问题。\n由于eventloop负责I/O操作，关系到请求的响应性，故不能在eventloop中进行过于耗时操作，否则会影响到其他绑定到该eventloop的channel，导致响应时间过长。耗时的操作可以使用DefaultEventLoopGroup业务线程池。 当然滥用业务线程池也不好，会造成过多的Context Switch（上下文切换）。\nNioEventLoop源码 想要有更深的理解，去读源码吧！\n -- 主要方法 1. register 注册channel至selector并且与eventloop绑定，每个channel只能由一个eventloop处理，每个eventloop可以处理多个channel。\n2. run selector模式轮询已经准备好的I/O事件并处理以及任务调度等。\n3. processSelectedKey(SelectionKey k, AbstractNioChannel ch) 处理selector找到ready options（如write,read,accept,connect）。其中可以看到，最终处理都是由NioUnsafe完成的。\nOP_CONNECT的处理：由于Connect事件一般发生在Client，需将OP_CONNECT从selector感兴趣的事件中删除，避免出现selector.select一直返回OP_CONNECT， 触发ChannelActive，然后将OP_READ添加到selector感兴趣的事件中，selector继续轮询。\nOP_WRITE的处理：当发送缓冲区空间满的情况不能write，则注册该事件。当缓冲区有空闲空间则触发，强制flush并清除OP_WRITE，防止重复触发。\nOP_ACCEPT的处理：触发ChannelRead/ChannelReadComplete，创建新的channel并分配给child eventloop。\nOP_READ的处理：触发ChannelRead/ChannelReadComplete，读取消息。\n4. runAllTasks eventloop除了要处理I/O事件，还需要处理系统任务和定时任务。这些任务主要通过是execute方法放入task队列中。\npublic void execute(Runnable task) { if (task == null) { throw new NullPointerException(\u0026#34;task\u0026#34;); } boolean inEventLoop = inEventLoop(); addTask(task); if (!inEventLoop) { startThread(); if (isShutdown() \u0026amp;\u0026amp; removeTask(task)) { reject(); } } if (!addTaskWakesUp \u0026amp;\u0026amp; wakesUpForTask(task)) { wakeup(inEventLoop); } } inEventLoop()方法判断当前线程不是eventloop所对应的线程。将task放入队列是为了确保只能被eventloop对应线程执行，这样做是为了减少线程间的竞争。\n5. select(wakenUp.getAndSet(false)) 在for循环中，select循环ready I/O事件。\nselect(wakenUp.getAndSet(false)); if (wakenUp.get()) { selector.wakeup(); } wakenUp标识selector是否应该从Selector.select阻塞操作中被唤醒。false则使用需要使用Selector.selectNow。\n一般来说，wakenUp.compareAndSet(false, true)应该要早于selector.wakeup()（参考NioEventLoop.wakeup）以减少wakeup开销。 但是当有wakenUp设置为true太早了，会出现两种情况：\n Selector is waken up between wakenUp.getAndSet(false) and selector.select(...). (BAD) Selector is waken up between selector.select(...) and if (wakenUp.get()) { ... }. (OK)\n  情况1，根据selector.wakeup()方法注释：\n If no selection operation is currently in progress then the next invocation of one of these methods will return immediately unless the {@link #selectNow()} method is invoked in the meantime.\n 可知， selector.select(...)立即唤醒直到下一次轮询wakenUp.getAndSet(false)，wakenUp.compareAndSet(false, true)失败会导致NioEventLoop.wakeup失败，无法唤醒Selector，造成不必要的阻塞。\n故在select完成后如果wakenUp为true立即wakeup，下一次selector.select(...)会立即返回。（我感觉没有解决上面的问题啊！！！）\nlink  NIO基础\n Java NIO浅析 Reactor三种线程模型 netty性能优化 "});index.add({'id':14,'href':'/posts/juc-aqs_new/','title':"深入理解JUC：AbstractQueuedSynchronizer",'content':"AbstractQueuedSynchronizer 简称AQS, 是 JUC 并发框架的基石，支撑着 Lock 和 同步器的实现。其本质通过名称也能得知一二， Queued，通过队列实现同步，尝试获取资源的线程将封装成节点在队列中以自旋的方式获取资源，实现线程同步。\n在 AQS 中存在两种队列，同步队列和条件(等待)队列。同步队列中的节点是尝试获取资源失败的线程构成的，而条件队列中的节点是由于某些条件而挂起的线程构成的，类似 Object.wait/notify，当条件激活时，条件队列中的节点转移到同步队列并唤醒。\n1. 同步队列 当线程获取资源失败后会放入同步队列中挂起，等待下一次唤醒后尝试获取。其中节点有以下属性：\n   属性名称 属性描述     int waitStatus 节点状态   Node prev 前驱节点   Node next 后继节点   Node nextWaiter condition队列中后继节点   Thread thread 入队列时的当前线程    其中 waitStatus 有多个状态，非负数的状态意味着该节点不需要唤醒。\n CANCELLED（1）：节点由于获取超时或者被中断以至从同步队列中被取消。被取消的节点状态不会再改变。如果为取消状态，则不会被唤醒去尝试获取锁\n SIGNAL（-1）：如果节点状态为SIGNAL，则在其 release 或者 cancel 时需要unpark唤醒其后继节点。为了防止并发 release 或者 cancel 导致重复 unpark CONDITION（-2）：当前节点目前处于 condition 队列中，结点的线程等待在 ConditionObject 上，当其他线程调用了 Condition的signal() 方法后，CONDITION 状态的结点将从等待队列转移到同步队列中，等待获取同步锁\n PROPAGATE（-3）：与共享模式相关，在共享模式中，该状态标识结点的线程处于可运行状态\n NONE（0）：初始状态\n  head 节点代表着正占用资源的线程，next 节点代表着等待获取资源的线程，按照 FIFO 的原则唤醒非取消状态的下一个节点尝试去获取资源。\n2. 条件队列 条件队列是单向队列，结构在 ConditionObject 中有定义：\n// head 节点 private transient Node firstWaiter; // tail 节点 private transient Node lastWaiter; ConditionObject 类实现了 Condition 接口，该接口定义了与 Lock 锁相关的线程通信方法，主要分为 await 和 signal 两大类。\npublic final void await() throws InterruptedException { // 线程是否已被中断  if (Thread.interrupted()) throw new InterruptedException(); // 向条件队列尾部增加节点  Node node = addConditionWaiter(); // 释放资源占用  int savedState = fullyRelease(node); int interruptMode = 0; // 检查是否在同步队列中  while (!isOnSyncQueue(node)) { // 线程挂起  LockSupport.park(this); // 1. 如果由于中断而唤醒，  if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } // 阻塞获取资源  if (acquireQueued(node, savedState) \u0026amp;\u0026amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // 清除取消的节点  unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); }"});index.add({'id':15,'href':'/categories/','title':"Categories",'content':""});})();